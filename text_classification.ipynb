{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da929134",
   "metadata": {},
   "source": [
    "# Sponsor content detection in YouTube videos\n",
    "## Transfomers for binary text classification\n",
    "This notebook seeks to accomplish the task of sponsored-content detection using a binary text classification model. The text classification model is created by fine-tuning a DistilBERT pre-trained model.\n",
    "\n",
    "## Motivation\n",
    "Several similar projects based on a BERT-type text classification model have been written about in on the Internet. Unfortunately, in both instances the authors do not share details about the performance of the model. Instead, they used vague language like \"95% accuracy\" without qualifying that in any meaningful way. What is more, the trained models in both instances then demonstrably perform poorly in the downstream task of task classification, but no exact numbers are reported. \n",
    "\n",
    "We wanted to investigate how well a text classification model can perform on what is essentially a span extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eca0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, IterableDataset, IterableDatasetDict, ClassLabel, load_dataset, load_from_disk, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.realpath('..')))\n",
    "from data_loader import load_examples_from_chunks, load_captions_from_chunks\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1326632",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "Read the transcripts from the `data.N.json.gz` and extract examples using `load_examples_from_chunks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f452476",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    'content': 0,\n",
    "    'sponsor': 1,\n",
    "}\n",
    "\n",
    "def load_examples(chunks=None):\n",
    "    for example, label in load_examples_from_chunks(base_name='data', root_dir='./', chunks=chunks):\n",
    "        yield example, LABELS[label]\n",
    "\n",
    "def iterable_to_pandas(columns, iterable, max_length):\n",
    "    from tqdm.auto import tqdm\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    for item in tqdm(iterable, total=max_length):\n",
    "        df.loc[len(df)] = item\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3735d3",
   "metadata": {},
   "source": [
    "# Save prepared data to disk\n",
    "The dataset returned by `load_examples_from_chunks` is much smaller than the original ~10 GiB dataset because it does not include full video transcripts. We read this whole thing into memory into a pandas `DataFrame` and then save it to disk for further use. Loading the dataset into memory makes it easier to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bfcba7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mFound ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.12.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.13.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.14.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.15.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.16.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mFound ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"a sponsor of this video I work with pet flow because I think they really do offer a way to make your life better and easier so needless to say I think you should get your dog food from pet flow the great thing about them is that you can go and you order your dog food one time and then it's just automatically there whenever you need it you just select how often you want to deliver they save you the hassle of having to drive to the store every week or two to get your dog food I love that they and you guys support content like this because I think it's so important now I'll have their link in the description along with a coupon code that will give you an awesome discount on your first order did you know that\", 1)\n",
      "(\"puppies and their parents by making a contribution of any amount you'd like to our patreon campaign setup automatic pet food delivery with Peplow I'll have a link in the description as well as a coupon code that'll give you a terrific discount on your first order see you guys in the next episode of the dog training revolution good job bb-8 is awesome if you have a puppy you'll definitely want to check out these videos plus the training playlist I mentioned earlier that playlist will be in the description thanks for subscribing and thanks to all of our patrons on patreon we'll see you guys in the next video\", 0)\n",
      "(\"video which will almost certainly be d monetize this brought to you by honey when I shop online I don't want to feel like I'm wasting money and that's where a honey comes in honey helps give me peace of mind by scanning the internet for the best coupon codes that are available and immediately applying them at checkout it works in over 20,000 websites like eBay Aliexpress Sephora Newegg and more and it takes zero effort to install just go to join honey comm slash weighing and two clicks you're ready to save money online whenever you shop i've personally saved a lot of money over the years when shopping for pre-workout supplements and things like that that stuff can add up to a lot of money and it's always a relief to see honey bring the price down so there's really no reason not to start using honey today it's free to use and easy to install in just two clicks just go to join honey dot-com / Wang let's join honey comm / Wang I once again thank you honey for sponsoring this video because I'm definitely gonna need it for this one all good meat\", 1)\n",
      "(\"matter adding Ward out is the claim that popped up in the original thread that I have not been able to verify if it was there it might just be gone from the internet now but there was a claim that the original image could actually be traced back to a French website that had nothing to do with double dick dude but ultimately the jury is out as to whether or not this is a legitimate story and ultimately we wind up with three different schools of thought in the story you have those who think that everything he said is true those who think that he really did have two dicks but a lot of his story is embellished and a third faction that thinks he's just completely full of I'll put a poll up to see what you think as for the man himself it seems that he is still around and he's currently working on a comic book detailing his adventures but anyway as a now that's the story of double dick dude if you like this story you'll probably also like my video about two girls one cup peace out\", 0)\n",
      "(\"speaking of America the great American debate continues this weekend as seer and I will be going to Philadelphia this Saturday and in Washington DC this Sunday the show is an actual debates here and I have with each other using your questions we pick a moderator from the live audience there's heckling and of course there's a big long meet-and-greet after every show so get your tickets right here there's a link in the description and later this month we'll be going to Chicago and Detroit that's enough plugging\", 1)\n",
      "(\"may the 4th of July be with yesterday I asked you to describe America in just four words so here we go again 2016 edition forgot the boat well macking cheetos the most American thing I've ever seen mostly weebs and rednecks not Alaska and Hawaii very true they didn't they do not count the people of walmart ChristianMingle the movie yeah that about sums it up orange men wearing wigs hey Trump jokes aren't funny the first-ever\", 0)\n",
      "('head on over to g2a and enter commies crystal cave to get some sweet deals on crystals and more remember to click on the lowest price and use the promo code commie vs to get the best', 1)\n",
      "(\"Twitter how long is I how long have I been here hey thanks for watching I'm not gonna be one of those people who asked you to like and subscribe so I'm not gonna ask\", 0)\n",
      "('the kickstarter for dungeons of drakenheim is now live after 52 episodes of their live campaign the dungeon dudes decided to make a source book for you guys to explore the city of drakenheim was destroyed 15 years ago by falling stars and is now in a state of dark magic turmoil as a result of powerful debris this fifth edition source book will contain dozens of monsters spells magic items exploration mechanics and delirium crystals these celestial remnants contain entirely new volatile magic that will determine what the ruins of drakenheim will become in addition to the actual shards we also have an eldritch fog called haze that can contaminate and even mutate players and monsters alike go check out their campaign by clicking the link in the description give those dungeon dudes some love and explore their setting in a completely open-ended adventure thanks to those guys for sponsoring the video which i hope you enjoy ok so', 1)\n",
      "(\"bottlenecked the sheer amount of reinforcements should exhaust the party's resources if the party gets overwhelmed the guards will stop at around half their hit points and simply ask to place manacles on them before things get worse if things get worse the manacles come out after a concussion the guards could also employ more abstract ways of subduing crime ranging from the perfectly humane man catcher to powders that overwhelm the senses or just straight up sleep magicians for hire and of course the aftermath this is mostly up to you but a few things could happen a bribe could get the party off easy they could enjoy a quaint court visit or just be instantly sentenced based on the crime who needs eyewitnesses if you're not even from around here well enjoy this bit of forbidden knowledge and i bet this little research project of mine will rip up a lot of tables i did good here today thanks for watching\", 0)\n",
      "('To begin with, for anyone', 1)\n",
      "('life.', 0)\n",
      "('this video is sponsored by the rich the only wallet suitable for a certain cyborg general your Ridge wallet so this', 1)\n",
      "(\"battlefront three legacy that mod for battlefront true that's attempting to recreate the 2008 battlefront free game we never got in the mod you can play as General Grievous and today I wanted to see if\", 0)\n",
      "(\"that's fine speaking of the internet let's hear a word from today's sponsor expressvpn folks the internet is awesome but it can also be very dangerous if you don't take the necessary precautions every time you connect to an unencrypted wi-fi network your passwords and financial information can be accessed by hackers your internet service provider can see all of your browsing history and sell it to ad companies and your internet service provider can do this even when you're connected to your encrypted network without a vpn it's all just wide open to these jerks using the internet without expressvpn is like taking a personal phone call on speaker on like a crowded bus so everyone around you can hear your personal details and nobody wants that okay especially you luckily expressvpn encrypts your data through a secure tunnel so nobody can see who you are or what you're up to also expressvpn's encryption is so strong that it would take a hacker with a super computer literally billions of years to crack expressvpn also masks your ip address by rerouting your connection through one of their 3000 plus servers and with servers in 94 different countries you have a wide variety of places you can choose to appear from and while we're on the topic of appearing from other places on top of providing you with safety and privacy expressvpn also allows you to vastly expand your entertainment library let me explain say for example you're in the us and you want to watch prison break on netflix will you be out of luck it isn't on american netflix but it's on canadian netflix it's also on the uk netflix so i have to do is change your server to one in canada or one in the uk and there you go dude you're watching prison break still not convinced listen up expressvpn is consistently faster than any other vpn provider they have 24 7 customer support they're super easy to use you can connect with just one simple click and it's the top rated vpn provider rated number one by cnet the verge wired tech radar and many more and believe it or not it gets better expressvpn are hooking you up with a great deal find out how you can get three months of expressvpn for free by visiting expressvpn.com curtistown or clicking the link in my description below it's really that easy expressvpn.com curtistown or just click the link in the description all right thank you so much to expressvpn for sponsoring this video and so many others in the past i've been using expressvpn every day for the past like two years and i honestly cannot recommend it enough and also when you check out the sponsors it helps me out a bunch as well so everybody wins here man all right thanks so much back to me all right thanks so much for\", 1)\n",
      "(\"hey guys before the video starts just want to say i'm going on tour again finally this feels crazy to say but with vaccines rolling out and everything and you know life sort of getting back to normal i got shows coming up at the end of the year i will be in raleigh north carolina huntsville alabama nashville tennessee boston massachusetts bridgeport connecticut new brunswick new jersey and levittown new york some shows are already sold out because i announced this last week on my twitter and instagram so we added second shows but we may still add more so if you want to stay in the loop follow me on twitter on instagram and i'll be posting about shows on there link in the description or just go to curtiscar.com grab your tickets and i'll see you at the end of the year enjoy the video alright hey guys welcome back to my channel if you're new here what's up how's it going and if you're coming back what's up how's it going it's really good to see you again i hope you're doing well you see what happens when you subscribe to my channel you get an extra greeting at the beginning so do it uh pay no attention to uh my appearance right now please do not perceive me i mean watch the video and you know like it and stuff but reception is off limits all right i will say if they do a joe dirt remake dude i'm a shoeing folks how do you think the world's gonna end climate change alien invasion maybe a big shark i don't know nobody knows how humanity will come to an end i mean it's 100 gonna be climate change but hey you never know could be a could be a big shark the whole concept of the apocalypse is insanely popular in modern media and even ancient media in the good book not twilight the other one the bible they talk about how armageddon is the place where the kings of the earth under demonic leadership will wage war on the forces of god at the end of history but hey man if that happens armageddon me a cold one and a lawn chair because that sounds kind of full transparency here i also thought the term armageddon came from the bruce willis movie so but now i know that's not true i learned something new and that's a good thing so you can't make fun of me for it speaking of movies there have been a staggering amount of movies that take place either during the apocalypse or after the apocalypse also a lot that take place before the apocalypse that's like pretty much every movie i guess no there's independence day the day after tomorrow war of the worlds war of the worlds tom cruise edition this is the end the happening the core snow piercer i am legend straight up this whole video could just be me listing off\", 0)\n",
      "(\"to find also today's video is very kind of sponsored by adam and eve i haven't sponsored my videos for a while i love them and you guys love them so give them some love if you want me to continue show them to you guys on your channel if you want them to keep on supporting this channel so you can use code angelica for percent of one item with free shipping to the us and canada some exclusions apply they've been in business for over 50 years if 24 7 customer service 20 of all profits go to fighting hiv around the world 90 day no hassle returns discrete packaging and many many more so once again code angelica for 50 off one item of free shipping to the us and canada some exclusion supply and let's get into the video thank you to any for sponsoring and let's get into it so we\", 1)\n",
      "(\"shane dawson and david dobrik the duo making a comeback so shane dawson recently obviously has been caught posting on instagram a lot and i've been covering all of that just you know trying to decipher when he's going to come back it all started out with some you know instagram stories that was kind of like you know it's there for 24 hours and it disappears so it seems a little bit less like pressure for him if that makes sense to post stuff you know about his previous launch or the conspiracy palette and just like congratulating people i remember he congratulated trisha paytas on their engagement and obviously that then fell apart their friendship fell apart shortly after then he would post on his youtube community tab i really edited one of ryland's videos and then he posted\", 0)\n",
      "('this video is brought to you by sportlin quality integrity and tradition well while doing a', 1)\n",
      "(\"uh if you're interested go check out the hvac overtime channels give it a subscription you guys will see us pop up every friday we do shows so that's pretty much it we will catch you on the next one okay\", 0)\n",
      "(\"We don't have much time for this but still it's the holiday season So there are two things you need to keep in mind first. You want to look good? For this I suggest one of my sweatshirt and second you want to be nice to people which often includes let's face it Bribing them with gifts, All the informations you need our down below. Back to\", 1)\n",
      "('First temperature needs to go down to 28 degrees Celsius or 82 degrees Fahrenheit and then back up to 32 degrees Celsius or 90 degrees Farenheit Chocolate is at perfect working temperature at the moment All I need to do right now is to get it out, cut a slit and pour that in those molds', 0)\n",
      "('to assembly. Before I do that. Let me tell you about our sponsor. - Private internet access has a VPN that encrypts all of your internet traffic. And uses a safe protected IP. Connect up to five devices. Windows, Mac OS, Android, iOS, and Lennox. Featuring an internet kill switch. That keeps you in control of your connection. Try risk free with a 30 day money back guarantee at the link', 1)\n",
      "(\"Really need to find myself more of these kits, so I can build them myself. Like we have a laser here, I have my laser. MDF is cheap. And all these parts are really cheap. I always found the problem with kits like this is you build it once and then you're like, oh this is neat. I'll use it for a week and be really, really happy with it. And then it ends up on a shelf and\", 0)\n",
      "(\"no it's not today the other day i carved a whole chicken in the break room and now doug won't stop asking if it's thanksgiving so to get ready for thanksgiving i wanted a set of knives that were so well crafted they could have been made by hattori hanzo so i'm using kamikoto knives the sponsor of this video each knife comes in this heavy duty ash wood box to store it safely and it just feels special when you open it and when i carved that chicken it sliced through the meat like butter and perfectly balanced as all things should be the knives are created by tapping into 800 year old traditional japanese techniques sword smithing iron working and metal casting each blade is made of japanese steel using techniques that have been perfected by generations of knife smiths to shame ultron didn't ask him to make his robots maybe then they wouldn't have fallen apart like a wet sandwich all of these knives have gone through a rigorous 19-step process that takes years to complete and each set comes with a lifetime guarantee no wonder these knives are used by chefs at michelin star restaurants so kamikoto is offering our subscribers an extra 50 off any purchase with the discount code screen crush so click the link in the description to take advantage of this offer back to ultron you need patience need to see the big picture when he faces the\", 1)\n",
      "(\"and since ultron is an imitation of stark he himself suffers from the same obsessions that eventually lead to failures as a villain in the end it's our failures that tend to push us forward to learn from our worst mistakes and find paths that lead us to success ultron leads to zemo to civil war to thanos to the snap and to everything beyond that all of these failures are ways for the heroes to learn and improve and become better so maybe el tron was never meant to be a truly menacing villain perhaps his whole purpose was to challenge the avengers on a more personal level plant the seeds for the conflict of civil war prove to them that they are liable and hypocritical i suppose we are both disappointments i suppose we are still things would have worked so much better if ultron had a more intimidating presence in the movie thankfully the mcu fixed some of ultron's missed opportunities with what if but let me know what you think of ultron down in the comments or at me on twitter tell me how you think he could have been a more intimidating villain and if it's your first time here subscribe and ring that bell for screen crush i'm ryan airy you\", 0)\n",
      "(\"cool today i have partnered up with youtubes i haven't actually unboxed him yet i've been saving him for this very special moment this is the first time i've received the actual box and the actual figurine and i'm so excited we've been working on this for months it took like a year to even get one of these so we have the box here and it's that old vine design not the app i last longer than seven seconds i've done a load of little messages all over the box but i'm not going to tell you those yet no spoilers look how cool he looks i honestly i can't explain to you how exciting this is for me here you go here's little me i'm actually up there with the tallest of the youtubes so basically i'm taller than george and alex that's the only real important thing to say here i just love how intricate the design is from the very fluffy hair to the beard that doesn't actually connect up we'll get there soon down i promise and then you've even got clive on like who is clyde look guys we're doing the thing as i said i've been waiting ages for this and the big day is here we actually dropped the figurine today it's limited edition once it sells out it will never be on sale ever again you can find the figurine over on youtubes.com or you can just click the link in the description or the pinned comment i'm making it really easy for you it's a really cool thing to have been involved in and every sale directly supports me so i'd really appreciate it and thank you for putting me in the position where i can have opportunities like this this is honestly nuts so i figured we should start from the beginning because i saw oh\", 1)\n",
      "(\"here or there or just write yourself in general just pose a bit you really want to post a tick-tock no not watching that one it reminds me too much of my childhood again it's this whole doing an ugly face and a funny voice it reminds me of times that i simply do not want to go back to get on follow me on take that right now bro please what what is this it's not whatever this is oh when she posts that fire ass instagram that fish looks lit oh that is like oh oh i don't care what you say at home i put myself through a lot for these videos i put myself for a lot for you i feel like people just want to hate on her because she's living better than them maybe you're right maybe i need to start living better for myself and get that disney cash if only there was a way that i could do that only there was a way that people could help me live better and it was right in front of me and i could almost taste it i guess what i'm trying to say is if you want to support the james marriott channel which has been free to watch for as long as you've been watching it go ahead and pick up this lovely little boy i'd really appreciate it and thank you once again for watching this video if you did enjoy it leave a like subscribe please i want more subscribers and bye\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"all right baby we back i want to give a huge shout out for rayconf for sponsoring another video i mean we already know i use my raycons every day at the gym just like in league we stay sweating at the gym and instead of you know rocking that dog music to jimmy plane i rock my 90 boomer tunes all day like a league we stay sweating at the gym and with their new rubber oil look and feel these bad boys damn that's what we like to call nice even when we're sweating bricks they don't fall off not only that but they're half the price of other premium brands out on the market we're talking about 32 hours of battery life eight hours of play time plus you got a built-in mic to answer calls you guys see me at them squats for big numbers baby big numbers look at the gel tips look how perfectly they fit in my ear bing bada boom all you got to do is go to the website link down below in the description pick a color add it to the card and boom 15 off just like that thank you again raycon for sponsoring this video and thank you for supporting me when you buy a pair of raycons peace lightbobbybyraycon.com slash trick to gee 15 you\", 1)\n",
      "(\"don't all right we went all right let's get that burn let's look at that bearing get juiced the out of our goddamn sockets you guys knocked on the call victory yes we won again no are they seriously wanna fight this\", 0)\n",
      "('this video is sponsored by expressvpn a vpn is a virtual private network which is a secure way to keep your data safe on the internet expressvpn encrypts data which protects users from the threat of hackers who aim to steal private information wherever you are a fantastic feature of expressvpn is that users can change their ip addresses to places all over the world heading their own ip address in the process this is a fantastic way of watching netflix programs or films which are not available in your region for example studio ghibli films are not available to stream in the us but with expressvpn you can change your router location to the uk in order to gain access to the content personally i love to watch vikings and instead of paying extra for amazon prime to access it i can watch it for less on netflix by changing my location on expressvpn from the uk to australia expressvpn is fast easy to use and the number one rated vpn provider by wired and tech radar customer support is also available 24 hours a day seven days a week find out how you can get three months free by clicking the link in the description box or going to expressvpn.com dark curiosities', 1)\n",
      "(\"old she has never been located police suspect that sullivan is most likely deceased possibly buried in the polar mesa area at the time of her disappearance danny sullivan was 15 years of age standing at approximately five feet tall and weighing 98 pounds she is of caucasian descent and had brown just above the shoulder length hair and brown eyes denny had also previously suffered a fractured leg and she was also known to wear reading glasses at the time she was abducted she was last seen wearing a blouse slacks and a pair of red canvas shoes the case remains unsolved and one of utah's oldest missing persons cases if alive today denny sullivan would be 74 years old you\", 0)\n",
      "('this episode of shadow versity has brought to you by the longsword shirt available through teespring link in the description', 1)\n",
      "(\"i'm shad and the classic medieval cruciform styled sword in its one-handed and two-handed variations is grossly misunderstood\", 0)\n",
      "(\"miss a thing but first let's hear about today's video sponsored if you haven't heard about the extremely popular raid shadow legends by now it's a turn-based battle mmorpg meaning you take turns pitting your characters or champions against those of your rivals the bosses and raid are fantastic and strategy is key to defeating them for example sylvania guardian of the spirit keep was betrayed by the elves of arabia when off on her own and doesn't want to share the magic of the spirits anymore to defeat her you'll need a couple of champions that can put out healing reduction to stop her healing and bring someone who can remove buffs to strip her defenses most of the games i usually play don't get updated often so i can really appreciate how raid is constantly adding new things and events to the game for this month raids got a non-stop jam-packed halloween lineup towards the end of the month we're talking big rewards tournaments fragment events to get some brand new legendary champions including one very spooky halloween champion and much more if you want a head start with the game hit the link in the description or scan the qr code on screen new players will get these rewards and the epic hero chinoro for free new players will find their rewards here in the inbox for the next 30 days only thank you very much ray for sponsoring this video now let's get into the epic airsoft gameplay you've been waiting for for the first two games of\", 1)\n",
      "(\"go that worked that worked really well cartel all right dead you're very welcome in the next video we'll chronologically pick up right after this with more awesome footage from this epic day if you're new here subscribe make sure your notifications are set to all and i'll see you guys next time have a good one you\", 0)\n",
      "('In the early 17th Century, a religious man decided he', 1)\n",
      "('going to uncover the architecture', 0)\n",
      "('legendary court of Rudolf II. The Court of the Golem If', 1)\n",
      "(\"going to uncover the architecture of the universe itself. Using observations of the Heavens and a deep understanding of geometry, this man conjectured a world of harmonious design, where the secrets of God's creation could be deciphered by anyone willing to use mathematics to look for them. In the course of his quest, this man managed to change our understanding of astronomy forever. His name was Johannes Kepler, and he would go down as one of the greatest scientists to ever live. Born into a divided Europe, Kepler grew up during a time of unfathomable change. Witchcraft trials, religious war, and a scientific revolution were all pulling the continent apart. Yet Kepler managed to not just navigate through these changes, but to do so while transforming science. He discovered laws of astronomy, changed our understanding of optics, pioneered crystallography, developed telescopes, and even wrote what's likely the first work of science-fiction. Persecuted for his faith, misunderstood in his lifetime, this is the life of Johannes Kepler, the man who discovered the secrets of the universe. Children of the Revolutions When Johannes Kepler was born\", 0)\n",
      "(\"I finally got one I've been a favor for so many years now hey does anyone know where the jar store is my hammies you to sustained-release today like right now just click the link in the description it will take you directly to my youtube look I have one right here please take care of it it can even uh um do a backflip mine do it I don't sell any merchandise at all so this is very special to me today is the only day you can pre-order but be quick there are only a limited amount can you hear that\", 1)\n",
      "(\"the future is now Old Men the future is now but we're so alone but now you can hug someone from over the internet and feel it I came across a YouTube channel called I am lucid and first saw his video could people in via chat touched me and thought oh my god I want that so I reached out to him and he was able to get me in touch with the big haptics team so all credit goes to him B haptic sent me a headpiece vest and arms the first time I wore it was on my live stream at twitch.tv slash Hamby do you like this frog but first I invited my friend to kick me in the face you can kick me in the face I would love to please come\", 0)\n",
      "(\"say thanks to our sponsor nordvpn listen in my line of work i have the 50 cent army after me i have the chinese government after me people trying to compromise my internet situation and my accounts so of course i use a vpn and the vpn i choose to use is nordvpn now a vpn is really cool what it does is it encrypts all of your data and makes it safe and secure so that no one can see what you're doing online but it's not just my special circumstance that requires a vpn anybody using any sort of wi-fi or unsecured internet anywhere can benefit from using a vpn and i recommend using one all the time nordvpn is the easiest to use you can hop around to all different servers around the world and you can even get past something called region locks which is where you're kind of locked out of a certain country's content because you're not in that country well with nordvpn you can actually go to that country's internet and see that content it's really really cool don't forget to go to nordvpn.com you'll get a massive discount off of a two-year plan and you'll get four free additional months all right all right so\", 1)\n",
      "(\"to these companies if you're self-censoring for china right now you might want to stop because like eventually it's going to stop being excusable like eventually the customers in your western markets are going to be well-versed in what's happening in china the atrocities human rights atrocities that the chinese government commits they're going to be well versed in that enough to where they're not going to think that that's okay and they will boycott you at least in this situation i think that they're having a little bit of fun with it anyway i want to say thank you to everyone out there appreciate each and every one of you especially you guys over on patreon.comlawy86 you guys are awesome you're the reason i do what i do you're the only reason i can do what i do especially with all the demonetization that i suffer because of the content that i cover so i really appreciate you guys supporting me over there and i want to say thank you so much a lot winners and i'll catch you on the next one you\", 0)\n",
      "(\"games so keeper security is the leading cyber security platform for protecting individuals families and businesses against password-related cyber attacks yup talking about hackers these attacks can lead to identity theft financial loss and reputational damage experts predict too that cyber criminals will increasingly target the gaming industry over the next three years so with keeper security you can create strong unique passwords not the old passwords you've been using since high school special ones for every account you use where you log in but you can also keep them stored safely in your own encrypted vault visit keeper dot io slash beast coast and use code beast coast that's in all caps beast coast for a 30 off promo code stay safe out there my friends the\", 1)\n",
      "(\"of ideal follow-ups you actually want to do and not like 10 or 15 things that people want to do like people will try to like get you to believe and by the way this is not to take away from dragon ball because dragon ball does have stuff that i like dragon ball does have stuff that i like the neutral at high level is very fun actually and like a nice level of stress okay i don't want i didn't want to go too deep in that tangent any questions any questions this one was um this is probably the thing that bothers me the most when it's like that how you guys feel how you guys feeling you\", 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('is sponsored by Squarespace the all-in-one platform to build an online presence and pursue your dream but more on that at the end of', 1)\n",
      "('launch go to Squarespace com forward slash history time or simply use the offer code history time to get 10% off your first purchase of a website or domain', 0)\n",
      "(\"suspicious speaking of smells though let's take a second and hear a word from the sponsor of this video now you guys know i love smelling like someone's expensive wife because that's what i am i am someone's expensive wife you know him his name's tony off-screen tony if you will now i love smelling expensive but i don't want to spend all that money on expensive perfumes so what's the best solution the sponsor of today's video which is scentbird if you don't know you most likely already do but scentbird is a subscription fragrance service that gives you the opportunity to shop over 600 that's a lot over 600 brands that's so many and that is a flexible subscription and it is monthly but no worries if you are like me and you have like 40 of these 40 of these little twist capsule guys in your cabinet back there you can just skip a month they come in these little containers little twist guys twist just like a lipstick fool proof to use and then you can just take out the little vial right here and it says exactly what the name is i'll go over what they smell like and which ones i got in a second but that's how you use it and i personally love that it's not like a tester one it's not like a travel size it's look how big look how big this is it look okay this is my face that's big and it says that it's a 30-day supply but like girl it's more than that okay if you have no idea what you're doing when it comes to fragrances to different scents what you should wear for each occasion what does it mean when it says that something smells more musky or has notes of lavender and lilac if you're completely lost don't worry they actually have a quiz on the site that is very easy to use and then that way they will point you in the right direction so you can smell beautiful and perfect and valuable spicy if you want you'll be able to get what you need and with the holidays coming up it is such a good present my sister doesn't watch my videos so it's fine that i'm saying this this is actually what i'm giving to her this year i'm gonna be gifting her a yearly subscription so you can actually do three months six months or 12 months there's perfumes there's colognes there's unisex options like i said it is the perfect gift for this holiday season and the great thing is that once you choose either the three month six month or a 12 month subscription plan go go ahead and send it right over to the recipient's email so that way they take it over from there and if you're thinking i don't know if i want to sign someone up for like a subscription service don't worry because scentbird actually has gift sets available for whoever so the ones they sent me this month were power suit by deck of scarlet i've already smelled this one it smells very powerful like i'm about to dismantle empires and aren't i aren't i going to do that and then light blue by dolce gabbana i feel like this is like this is like a staple like this is one that people like know a lot about know a lot about it's a perfume chelsea and then pure vanilla by a love vanilla this will be interesting because i typically don't like vanilla scents so it has vanilla helio heliotrope i'm pretty sure i'm pronouncing that wrong and then tonka bean so it's not just vanilla all right so here's that the la vanilla one oh my god that's really good oh my god it smells like my nana oh that's so weird one of the last ones i got smelled like my mom and that one smells like my nana okay i love that one like i feel like i need to write on the bottom smells like nana i'm such a weirdo okay and then dolce gabbana light blue i feel like i should say this dolce gabbana light blue like it doesn't in the commercials if people like swimming you know ooh that one has like a little hint of like citrus kind of oh my god i'm so smart oh we should make this like a game every time so this one has notes of lemon apple cedar bamboo and bluebell i know this is something probably people like don't show too much but like i love that it comes with these cards and like tells me exactly what's in there so i sound like i know what i'm talking about when i say i'm a perfume connoisseur like i said i just mean that i am a hoarder this one smells really really good it that one definitely is lighter than the other one oh my god that's why it's called light blue okay now power suit i already smelled and it smells real good like real good and it has notes of ooh creamy sandalwood opposed to normal sandalwood skin musk that's that's a gross word skin skin musk orange flower which tony's gonna love this one and jasmine he loves both those smells and cardamom i don't know what that means but i already know like this one's gonna be a little a little bit more on the spicy side that is so good oh my god this is what i would wear with a with black jeans heels a like loose silky top and an oversized blazer and a power pony and i am forcing multi-level marketing companies to shut down that's this is what i'm gonna wear i love this one okay so whether it's gifting a subscription or treating yourself with scentbird you can get a decent amount of luxury fragrances for just 14 a month also with my code cc suarez 3 which i'll have linked down below and in that pinned comment i'll have the link and the code you can actually get your first month for just eleven dollars yep that is saving you thirty percent my friend mm-hmm not only am i helping you smell amazing but i'm saving you monies while doing so thank you again snet bird for sponsoring this video and now let's go ahead and get into our debauchery all right so like i said\", 1)\n",
      "(\"when someone upgrades you make more money too so like i said i'm gonna have all of this linked down below i will find a lot of their internal documents as well and i'll have those linked down below even if i have to put all of this on like and you know screenshot it and stuff with those documents and put it on my own website on my blog i've done that before for like mon8 stuff and other documents that i wanted to be able to share with y'all so that's the selfish suarezes.com and it's on the blog site so i'm gonna have that there if needed if i can't you know get the exact links to things um so i'm gonna go ahead and do that but as you can tell it's a scam it's a scam please do not fall for this any little w fab ladies if you are in mone please do not join this scam you deserve so much better and like i said i think it just really shows to a t how these people treat each other how they don't care about anyone else but their own pockets and how they're willing to do anything for money they these girls have been especially jasmine and i'm doing a deep dive on her you bet your butt the fact that she has been showing such a luxury lifestyle yet she she fell seven ranks from executive director to whatever the hell she was like seven drinks below that that's crazy she fell all almost all the way down the compensation plan that's insane good god this is whack alright friends i hope you enjoyed um keep in mind these financial mlms are whack to put it nicely the reps in these companies not all of them but a lot of the reps in these companies will have your ins like they will master report your instagram account your youtube videos it's crazy they will harass you to no end so there's a creator amal williams she's amazing she's a canadian creator she is a beautiful mother wife a woman of color she's absolutely gorgeous and very funny and she made a couple videos about these these people in these companies and like more like financial mlms this company specifically igenius and that was months ago and she's kind of taken a break from youtube she's coming back though i just talked to her she's coming back and they are still harassing her on instagram and her dms it is absolutely it's it's strange behavior it is strange strange behavior so i'm actually right before i put this video up i'm gonna make a second instagram account i'm like i'm gonna have to have a backup and it's unfortunate that i have to do that but that is honestly one of the many reasons why i had been not really covering these types of financial mlms too much because i've seen what they've done to marco marco gets his instagram page taken down at least once a month and he has a backup and now his main account's shadow band which really sucks but hopefully that won't happen to me because i don't really go after them on my instagram i just promote my videos i will definitely go after them here because youtube has different guidelines thank god thank thank the lord youtube protects its creators better than instagram does so i will have all of them all not all of her videos but her channel linked down below i'm going to add her to if you've ever looked in my description box not only are there affiliate links and codes and stuff like that in there but there are there's just a list of some of my favorite creators and my fellow commentary creators as well um so i'm gonna add her to that i'm gonna have a few of her videos linked down below too and you you guys know marco he is hilarious he puts out a lot of content exposing these types of mlms as well so please go watch their videos amal is realistically one of the only people that i have seen talk about this mlm and she has been harassed for it a lot i just posted her page on my community page earlier today as i was filming this and asked y'all to go subscribe to her i really want because she is coming back to youtube i really want to show her support and to really help her because she really brought not only this situation to my attention like 50 of y'all did y'all were flooding or flooding my dms i was just trying to eat some peanut butter pie with my family how white is that peanut butter pie what and the cassidy is happening anyways i hope you'll have a great rest of your day i will have my backup account linked down below too now sounds so strange for me to say that i've been thinking about it for a little bit and i'm like dang i should probably get one so hope you'll have a great rest of your day please know how important you are how valuable you are how amazing you are we got some big things coming up not only in december but at the beginning of next year as well and throughout next year so i hope you'll have a great rest of your day weekend at night whenever you're watching this you better stay spicy that blue glowing light behind me it does not say stay sploy it says stay spicy if you want to smell spicy and powerful and beautiful and expensive use the code and the link in my description box and in the pinned comment to get 30 percent off of scentbird fragrance subscriptions and also again thank you scentbird for sponsoring this video and continuing to sponsor my channel and thank you guys for supporting my sponsors without you i would not be able to do this full-time i wouldn't be able to do this period without you and when you support my sponsorships it really just makes it so i'm able to put out even more amazing crazy content for you so stay spicy and i'll see you in my next video bye you\", 0)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "for x in itertools.islice(load_examples(), 0, 50):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27677b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = iterable_to_pandas(['text', 'label'], load_examples(), 16 * 20_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1d24e",
   "metadata": {},
   "source": [
    "NOTE: The above progress bar was out of 320,000 because that was the realistically maximum number of samples that we could get given the dataset that we have. The red color is not an indicator of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e464421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.from_pandas(df).remove_columns('__index_level_0__').save_to_disk('./classification-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99e322",
   "metadata": {},
   "source": [
    "# Read prepared data\n",
    "Read the prepared dataset using the  API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1791ca81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 112118\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12458\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_from_disk('./classification-dataset').train_test_split(test_size=0.1)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b79e1b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"illegal but before we do get started we have a sponsorship for today's video our first sponsorship on the channel and what better first sponsorship then braid shadow legends a game that I've been so addicted to that my girlfriend is threatening to break up with me because I refuse to put it down not going to lie a fluffy cartoon looking game here and there is no problem with me but why is every game becoming more and more cartoony I can't take it anymore I'm tired of all the candy all the unicorns all the bright colors I'm tired of it ok from now on you can miss me with all that I need an RPG game that's not afraid to get dark to get real to get raw but also one that's epic and awesome and guess what raid shadow legends is just that this game will take you to a ward of dark fantasy and realism now what do I like about the game you may be asking you know why am i promoting this game well for one the customization is out of this world I absolutely love it I love being able to customize literally everything I do the battle and the story is so epic that I literally can't put it down and the fact that they are constantly updating the game to a point where every time I pick it up I know there's something new for me to do you can play raid on both mobile and desktop because the game is cross save and if you don't know what cross save means well that means whatever you do on one platform will transfer to the other speaking of desktop I have to mention that oh boy this game graphical wise is stunning on PC and the best part it's one of the most well optimised games I have ever played on a PC it runs super fast and a smooth as butter baby now if you want to find your boy in game well I'll be under the name the big beefy boy and if you're quick enough you can also join my clan so what are you waiting for go to the video description click on the special links and if you are a new player you will get 100 thousand silver plus one free champion hex Weaver all this treasure will be waiting for you here and this reward will only be available for the next 30 days once again I would like to thank raid for sponsoring today's video and now let's get into the top 5 sidearms now some quick\",\n",
       "  \"my god it's 90 minutes boys allocate that time more to golfers and then he has another snack immediately after that so he's just snacking living it up he's definitely snacking in the cryo chamber like what else do you do in there is in there for an hour yeah look at this 11:00 a.m. family time it's like all right everybody gather up get around we got one hour well well y'all so careful he also has a sandwich in meetings and work calls so it's family time but he's on the phone the whole time they don't even get their own hour yeah but everybody let's take an hour for lunch yes Marky Mark he's like okay everybody they've lunch that we've made fun of a lot of people the four of us when we're all together but this is the first time I've been afraid that like they're gonna kick this door down and just blind in one eye but you guys are making jokes about me huh boys you see Planet of the Apes I heard you guys were mean he's got kind of a Mike Tyson thing with the realsense there was I heard you guys were making jokes about me I just want to tell you he has like the sense even I just want to tell you guys across that it's not cool that you guys making fun of me he looks like he's always explaining to a kid why they can't buy a present Wow really accurate I just don't get that right right no it's like your mom's gonna see you come back with it she's completely mad can I say something go I actually loved that shtick I'm very charmed by it yeah the sick being white his voice no his like sensitive tough guy I love that character I worked on a movie that he was in oh yeah tell me about the experience it was the lovely bones the loveliest lovely bones yeah I was like a an additional PA crew member a swing guy in there somewhere like a weekend but I got to see him walking around set I'll tell you he's always in a good mood he was whistling singing no cup of coffee in his hand just bopping along like the little swing really got a stain on it I really should have gotten this thing dry clean before the stain does not come with I don't think you need to dryclean that I mean so I so positive he probably goes in his trailer and like punches someone in the face yeah I lied you warned me about like eye contact or anything like that they'd hear about with celebrities but right right just looking back that's a weird that's weird require oh don't make eye contact like there's stories about that's appear insanity really super good actor who liked it Lincoln what's his name Daniel day-lewis no eye contact no the opposite eye contact only then like the best actor wouldn't need that for like his method acting it's like I get it you look him in the eyes he up his character but he was like between takes on Lincoln just like chillin with the crew and like telling stories and stuff in there you go alright no just out of character oh I was like alright let's go again I'm Lincoln Wow maybe they thought good he is better Pro it makes me doubt the whole eye contact thing that it's like a performance issue oh yeah there's actors being divas the whole point what do I know I'm just well I sorta mean I don't allow you to look me in the eye yeah but that's like yeah I'm just like whoa in careful with that I contacted I already warned you twice you just like alright we're gonna take a break fans yelling at me guys we're gonna take a quick break we'll be right back the boys are just back in town and we're only beginning we've only watched like one video of rigging a hundred thousand that's right we'll be right back stitch fix is an online personal styling service that finds and delivers\",\n",
       "  \"brought to you by viewer viewers bringing a new perspective to performance apparel perfect if you are sick and tired of dressing in traditional old workout gear it's so comfortable and then living your life yeah here's the deal i like to throw on workout clothes in the morning yeah i like to work out i like to then do a podcast go in have some lunch run over see leanne at the new house new podcast studio come back and then do another workout with vyori's core pro the core pants i can do all of that their comf their clothing is so comfortable and like i said i can work out in it my wife got the women's performance joggers the softest joggers you'll ever own and our girls fight over them they also have the daily leggings no no mistake in the name you will wear them daily they're high-waisted so isla loves them and they've upgraded to a no-slip fit thank god i have the core shorts and the ponto shorts ponto shirts are perfect for literally lounging around the home uh and and i i've actually slept in them they're that comfortable so good you're always an investment in your happiness our listeners they're offering you 20 off your first purchase get yourself some of the most comfortable and versatile clothing on the planet at fioreclothing.com bears that's v u o r i clothing dot com slash bears not only will you retrieve 20 off your first purchase but you'll enjoy free shipping on any us order over 75 and free returns go to yori clothing.com bears and discover the versatility of yori clothing this episode of two bears is also brought to you by tushy listen you know you look at me you know this guy knows how to take a dump and does he know how to clean up afterwards i'll tell you it's been a lifelong challenge a mess a disaster and thankfully thankfully tushy comes along the brand new hello tushy 3.0 modern bidet attachment is here to level the playing field the hello tushy 3.0 doesn't just cleanse your butt with a precise stream of water it cleans itself and listen this is a game changer man you end up saving so much what it reduces your toilet paper by 80 and there's nothing like dropping a big load in the toilet and then having a nice spray of water right into your hole ever since you introduced it to me i've been hands-free better for the environment i literally i pat dry i packed i tried clean water on my pat dry walk off i'm a huge huge fan go to hellotushy.com bears get 10 off your order and free shipping i mean i oh my\",\n",
       "  \"to be clear that I'm not suggesting at all that you should lie you're going to pay for the car in the end right that's all that should really matter to the dealership in the grand scheme of things that you pay for the car so what I'm saying is that you shouldn't mention how you plan to pay for the car the moment you walk in the door and don't start babbling about cash at any point during the negotiations don't play those cards until you're in finance before I explain what changed in the business model that made dealerships dislike cash a little\",\n",
       "  \"skip it up up and down up so I'm gonna take you on a trip down memory lane of the tragedy that will soon be the Atari VCS unless it gets too late again which it probably will it may get delayed again after that and then it may just cease to exist and never actually come to market but anyway let's assume it's coming out in March of 2020\",\n",
       "  'now learn even more about the cutting edge technology deployed above ground in the MagellanTV documentary Vietnam War. Part of the series, Combat Machines, this entry explores the battle of North Vietnamese ingenuity against the latest advances in US military power. MagellanTV is a new type of documentary streaming membership created by filmmakers that brings over 3,000 documentaries to all of your devices. Visit try.magellantv.com/darkdocs or click the link in the description below to get a free one-month trial. MagellanTV continues to add new and compelling documentaries in their war and military playlists, along with other feature genres such as true crime, space, and ancient history. Support Dark Docs and check out MagellanTV with a one-month free trial. Click on the link in the description below or visit try.magellantv.com/darkdocs',\n",
       "  \"Sponsored by Ayylien Clothing. Okay, let's umm,\",\n",
       "  \"So right before I get into the 5 cars I want to share something with you all that's going save you lots of money on holiday shopping. I mean who doesn't want to know how to save money. All you need is honey. What's honey? Honey is a free browser extension that automatically looks all over the web for the best promo codes when ever you're shopping online. This means you effortlessly get the best deals every single time on over 20,000 sites including very popular ones like Amazon, eBay, auto parts warehouse, Walmart, target, best buy and more! I actually have to buy a gift for my father, here let me show how easy you can save money. It's super easy to install. just visit joinhoney.com/vehiclevirals click add honey, confirm your selection and now you'll automatically be able to save money. I decided to get a few essential car cleaning products for my dad. At the checkout page, this dancing coin pops up. Hit apply coupons. And now you wait while honey searches for the best possible promo codes to save you money. And look at that, saved $25 for doing absolutely nothing. Guys there is no reason why you shouldn't get honey today, it's free and installs in just 2 clicks, plus imagine all the money you or a loved one can save while shopping for the holidays. Get honey for free at joinhoney.com/vehiclevirals that's join honey.com/vehiclevirals. Now kets get on with the\",\n",
       "  \"another reason why t-mobile is America's best unlimited network carry on my wayward son there'll be\",\n",
       "  \"cores now the information doesn't stop there nextgen is apparently set to get a massive jump in l2 cache we're talking up to 96 megabytes compared to six so 16 times the l2 cache which likely makes this nvidia's answer to amd's infinity cache either way the ad102 part gets 96 megabytes ad103 has 64. ad104 gets 48 and both the ad106 and 107 have 32. at the end of the day nvidia's next-gen gpus are shaping up to be a massive jump in performance hopefully they won't also be a massive jump in price so while that does it for today are you pumped for nvidia's next-gen gpus or are you just excited to finally be seeing price drops let me know down in the comments below and if you liked the video please subscribe and as always have a great day\",\n",
       "  \"fact that it's not even that laggy those mad damage and it kills that's fantastic movement that's gonna be in the conversation for one of the better uppers in the game man alright well file if it's incredibly fun guys as always make sure to subscribe and hit the bell if you want to stay tuned with more violence content there's a whole lot of videos that I'm planning to do stay tuned with all that also gonna be talking about buffs and nerves and this new passion a lot of things change this smash ultimate and I want to get to talk about all of that but that's the guys drop a like if you enjoyed today's video and let me down a comment down below what videos you would like me to do in this new patch what do you guys want to see most and I'll see you guys around tomorrow's video thanks for watching bye bye\",\n",
       "  \"are quite low. Before we get to number 1, my name is Chills and I hope you're enjoying my narration. If you're curious about what I look like in real life, then go to my instagram, @dylan_is_chillin_yt and tap that follow button to find out. It's a proven fact that generosity makes you a happier person, so if you're generous enough to hit that subscribe button and the bell beside it then thank you. This way you'll be notified of the new video we upload every Tuesday. 1.\",\n",
       "  'video is brought to you by Squarespace go to squarespace.com slash BSD and use the offer code BSD to save 10% off your',\n",
       "  \"before i continue on i want to give a quick shout out to my regular sponsor buksu if you all don't already know books will provide a gourmet experience of japanese snacks delivered to your front door they work with traditional japanese factories some over 100 years old to provide you with authentic japanese flavors and each monthly box has its own unique theme so you get different snacks each and every time first time users will get a seasons of japan box and after that you'll get a theme box like this one right when you open up the box you get this nice booklet that takes you through each snack as well as extra information about japan and japanese culture and they're all hand-picked from all over japan to deliver you unique and local japanese flavors so get 10 off your own japanese snack box subscription from boku and save up to 47 using my code paulo 10 and link in the description all right let's continue on with this\",\n",
       "  \"the fact in case people are worried about me or something i'm completely fine i've recovered now um and i got swabbed it wasn't you know that the bad thing um also thank you so much to crow and franz who kicked ass on this episode absolutely they just knocked it out of the park alright yay\",\n",
       "  'was fun and this video is sponsored by pcb way for this',\n",
       "  \"where that goes and that brings us to today's sponsor climate exchange common exchange is a nonprofit that is focusing on research advocacy and media support for local campaigns to pass ambitious science-based climate policy at the state level this work is super important because it helps States reduce their overall emissions which pooled together means as a country in the United States the biggest polluting or one of the biggest polluting countries in the world it helps us all just have a better future and fight climate change so I absolutely love what they're doing and I'm thrilled that they're sponsoring today's video and it's not just the mission that they have and all of that and how much I believe in that it's that they also are having a raffle where you can win some pretty awesome prizes the first-place winner can get their own Tesla they can build it choose the model options etc up to a hundred and forty thousand dollars in value and they will pay the taxes worth thirty eight thousand dollars on that so the winner doesn't have to which is great because otherwise even if you got the free car you still have to pony up all the taxes and that can be a lot now this basically gets you any model fully loaded excluding cyber truck second place gets $10,000 in cash third place $5,000 in cash combined pool cash pool of over a hundred ninety five thousand dollars for this raffle so if you want to go get some raffle tickets maybe win a Tesla and you regardless support these guys and what they're doing then go over to Tesla Mexico climate exchange I'll put a link in the description and thanks again for sponsoring the video so another company\",\n",
       "  'this video is sponsored by server Pro server Pro has everything you need to host the next big hightail server with custom host names and support for modding and scripting why not start your adventure today and grab a free minecraft server that you can swap over once hightail is launched the',\n",
       "  'it was to find that information in preparing this video we had to dig deep accessing thousands of websites some of which came off as a bit sketchy but i just discovered a tool which helps me to stay protected so i wanted to share it with you guys atlas vpn right now atlas vpn is running a huge discount meaning you can get a three-year subscription for just a dollar and 39 cents a month with a 30-day money-back guarantee but time is running out so get your deal by clicking on the link in the video description below and let me tell you this service was a total game changer for me as an american expat living in europe i enjoy the services blazing speeds and ability to access streaming services around the world as a video producer my team downloads endless amounts of media from all corners of the internet which would normally come with some malware risks however thanks to the fact that atlas vpn stops ads and malware on unlimited devices we have a total peace of mind i should also note that mrs socash saves cash while shopping online by getting the best deals regardless of her actual location so protect yourself by signing up for this amazing offer before it expires once more right now atlas vpn is running a huge discount meaning you can get a three-year subscription for just a dollar and 39 cents a month with a 30-day money-back guarantee time is running out so get your deal by clicking on the link in the video description below and now back to our lady liberty island has had many',\n",
       "  \"you don't want to do that okay we're gonna have extra ads and blue or red on red red yeah you'd read and then market we got to do further bags I'm nesting yeah I'm coming cuz I'm 11 let's do it I have 10 notes by the way I have okay pull up sleepy you don't need to go did I get it I got it blue it's not full me no I'm depositing yeah yeah okay that's first my god if we get this dude on the floor you can just just go just go go go okay okay I Spit I shot it no way dude let's go that's it go baby oh my god 14 days 14 hours late Olli this got to be the best ending ever that's gonna be one good YouTube video let me tell you that we didn't get rolled first we got rules 26th we ended up in the 26th place taking almost 14 hours not bad I guess for the first time but you know what they say right if you're not first you suck there anyways guys I hope you enjoy it thank you so much for watching this very long video all the way to the end and as always I will see you later\",\n",
       "  \"first two volumes in a single day remember you can see what it's all about yourself for just a couple bucks by buying it from book Walker with the promo code mother's basement be sure to act quick if you want to make that March 6 deadline for the poster contest or you can buy any number of light novel or manga ebooks instead there's a ton of great stuff to choose from with more being added all the time you know what else has stuff being at it all the time this very YouTube channel and you can make sure you catch all of it by subscribing to mother's basement and turning on notifications you have my gratitude if you do and of course as always I have to thank my patrons for supporting me in making all of these videos they wouldn't exist without you guys now if you want to hear me talk\",\n",
       "  \"about some of the design issues we got on this bad boy now if you're following on for part two you remembered me talking about one of the problem that I had with this design in that we're going bronze steel bronze steel bronze bronze steel as opposed to steel bronze steel bronze steel bronze steel bronze feel like we have a little bit too much bronze in this one area and we need break it up and that's where this piece that I've been holding in my hands for the last couple of minutes comes into play this is a little off cut of the Damascus that was used not only for the blade but also for these Damascus elements in the handle piece my plan is to turn this piece of Damascus into a spacer that goes between our flour element and our guard or quillion element just to break up the flow that design hopefully make it look a little more balanced to the eye so that is gonna be my task to start off with now as you saw in the last episodes we have this little small sandblasting unit and the plan is to use the sound blaster for the final finish on these parts you want to run through a little bit of the idea here and what it is that you're gonna do so this idea came from Neil's Vandenberg black dragon for two we designed this dagger with and who was just here in episodes 1 &amp; 2 he does this on a lot of his bronze fittings he sandblast sit down and then takes a steel blue and completely blackens it out and then can actually relieve some of that blackening with a little bit of steel wool so you've got black with highlights of bronze running through it gives it a little extra depth some it new colors and all of that exactly it looks really really nice so after I've gotten finished doing the final little touches up on these bronze pieces I'm gonna sandblast them down and then try to get a nice even finish with this super blue from birchwood casey so that's unscrew it and get to it\",\n",
       "  \"This video is sponsored by Epic War: Thrones  the new free-to-play, MMO real-time strategy game, set in the historical period of China's Three Kingdoms. While the Roman Empire was near its peak in the west, China was convulsed by one of the most turbulent periods in its history. This is your chance to take the role of a feudal lord in the Three Kingdoms era from your phone: waging a strategic campaign of annexation, alliance building and careful resource management Alongside tactical battles in which your generals can show off their moves Epic War: Thrones recreates the Three Kingdoms period with stunning graphics, including 3D terrain brilliant character animations... and changeable weather with tactical implications. You can download Epic War: Thrones now for free using the link in our video description. Real war, real epic. Thank you to Epic War: Thrones' for sponsoring this video. When\",\n",
       "  \"obviously all new car all new body panels look smart if you ask me and i like it in this matte gray paint the tail light's a bit like those on the revised e-class but what is this this is a fake rear diffuser and what is that and that bear with me don't know the kawaii sticker truth but we have the kawaii grass straw of truth which will reveal these are as fake as fake exhaust pipes can possibly get and wait a minute i'm not done fakery there oh yes mercedes what are you up to you just can't give it up can you fakery is absurd from the side the new c-class does look very s classy doesn't it it just does now this one has the 19-inch aloe wells which is the largest size you can get on the c-class they actually start 17 which would look really really small also this design of wheel with this black insert rim kind of makes them look smaller than they actually are what do you think about them let me know in the comments so this car has a\",\n",
       "  \"to current for sponsoring this video i love you current is the new way to bank it's the future of banking it's a mobile bank with a visa card as you can see here current gets you paid up to two days faster current is actually a real bank account with no hidden fees and no minimum requirement or whatever that thing is no minimum balance requirement i don't know how money works yet i probably spend too much of it but you know what the reason why i love current is because if i pay using the card i'm redeeming points that i can then redeem for cash current works with apple pay venmo google pay cash app venmo do i say venmo i can't ever remember all the different payment sites it's too much one of my favorite things is that you can deposit checks using the camera on your phone so much easier no need to go to the bank and do all this complicated stuff i hate that i hate that they also have accounts for people under 18. so if you're feeling like you need a bank account you're under 18 current's got your back it takes two minutes to sign up card ships for free you need a bank and you want it to be easy go to current link in my description check out current thank you current for sponsoring this video back to the fast food and the fun although my tummy already is starting to hurt and we've only been to one place we're off to\",\n",
       "  \"people who will continue to do things better than what I'm known for and that's why we're we're really connected and that's why I want to let you know\",\n",
       "  'This episode of Make It Real is sponsored by Monster Legends the very first game to feature the Hacksmith as a character Download today to help support the channel and get your very own Hacksmith monster Here at Hacksmith',\n",
       "  \"weird or bubbly or strange it felt like an updated or elevated version of the most advanced vw that they sell driving down the road it's sure to get a little bit of attention because it looks a little bit different but not crazy amounts as it still feels like a regular car out there on the road and not something that you know is going to really draw attention in a bad way so thumbs up to vw for the design here overall i really like the id4 and i think if you're in the market for an ev in the low 30 000 range the id4 should be something that you can consider and this is coming from a guy that's only ever owned tesla so if i'm impressed with it you know that if it's your first tv you're definitely going to be happy with it now another car by a different vw brand porsche also really impressed me and not too long ago i got to spend a day with the porsche taycan up in the hills of los angeles if you want to check that out you can look at the video over here if you're curious and of course let me know what you think in the comments down below as always don't forget when you free the data your mind will follow thanks for watching and i'll see you guys back in the next one\",\n",
       "  \"other but Before we jump into the video I want to go ahead and give me a quick shot to our sponsor for the day house party feel free to click the link down below to download the app you check it out yourself it is an app that's used for video chatting it's available on Mac Android other platforms to sum it up the best I can this app is used for video chatting and playing mini games with people it's so cute you could play heads up you guys remember that one that's a good one you can do trivia I decided to call my sister while she was cooking dinner and there was a trivia game so I said let's play it the first question that comes up is what's a group of ferrets call she goes oh of course that's called a business did you guys know that I didn't know that but she knew it right off the bat why do you know that I learned a lot of random trivia that my sister apparently already knew because she's all-knowing but it was a wonderful way for my sister and I to hang out even during quarantine we may be locked up at different houses but that doesn't mean that we can't hang out and still play games together house party is extremely easy to use and navigate and it's also really easy to set up you just sign up and go I'll definitely be using this app more to keep in contact with my family and if you guys are interested please feel free to click the link down the description below to check it out yourself and thank you so much epic games for sponsoring this video I really appreciate it and now time\",\n",
       "  \"do like and subscribe it'd be thoroughly appreciated just enough time to once again thank opinion outpost for sponsoring today's video remember click on the link in the description to start earning extra cash today i've been simon dan have yourselves a great day and i'll see you all tomorrow night where i'll be reacting to some comments live please do come and check it out but until then goodbye you\"],\n",
       " 'label': [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['test'][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9a9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we've arrived here, everything with the dataset is okay and it has been stored to disk. We\n",
    "# can drop the in-memory `DataFrame` we constructed originally. \n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa0868",
   "metadata": {},
   "source": [
    "# Tokenize inputs\n",
    "Tokenize the datatset with the pre-trained tokenizer. Sequences are padded to the maximum length supported by BERT and truncated if longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d15e678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4dbd35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbb670ce8c744409e4fa77919332aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/113 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbcede004d94eeeb68f91d11745385e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c1df44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_datasets = tokenized_datasets.remove_columns(['text'])\n",
    "train_dataset = cleaned_datasets['train']\n",
    "test_dataset = cleaned_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443c293",
   "metadata": {},
   "source": [
    "# Prepare for training\n",
    "Set training parameters, configure metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be75c0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-classification-uncased\", \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4,\n",
    "    save_total_limit=5, \n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10_001,\n",
    "    save_steps=5_000)\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    return {**accuracy, **precision, **recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adac00e",
   "metadata": {},
   "source": [
    "# Train the model \n",
    "We're using the default number of batches, but we terminate the training early because we observe that the model performs extremely well on all metric on the test dataset and because the training loss and validation loss are comparable after step 30,000, indicating that there is not too much over- or under-fitting, and that the model is not likely to learn anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87a44e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from distilbert-classification-uncased/checkpoint-30000).\n",
      "/home/veselin/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 112118\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 84090\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 1\n",
      "  Continuing training from global step 30000\n",
      "  Will skip the first 1 epochs then the first 1970 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8229cb2cfc44d9ea43b1657adb59994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84090' max='84090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84090/84090 7:28:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30003</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.040455</td>\n",
       "      <td>0.993739</td>\n",
       "      <td>0.994917</td>\n",
       "      <td>0.992710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40004</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.025425</td>\n",
       "      <td>0.995906</td>\n",
       "      <td>0.996195</td>\n",
       "      <td>0.995721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50005</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.033823</td>\n",
       "      <td>0.995264</td>\n",
       "      <td>0.995403</td>\n",
       "      <td>0.995246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60006</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.023235</td>\n",
       "      <td>0.995746</td>\n",
       "      <td>0.993221</td>\n",
       "      <td>0.998415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70007</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>0.997030</td>\n",
       "      <td>0.996517</td>\n",
       "      <td>0.997623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80008</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.997351</td>\n",
       "      <td>0.997149</td>\n",
       "      <td>0.997623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12458\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-35000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-35000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-40000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-40000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-15000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12458\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-45000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-45000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-50000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-50000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-25000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12458\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-55000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-55000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-60000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-60000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-35000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12458\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-65000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-65000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-70000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-70000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-45000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12458\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-75000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-75000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-75000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to distilbert-classification-uncased/checkpoint-80000\n",
      "Configuration saved in distilbert-classification-uncased/checkpoint-80000/config.json\n",
      "Model weights saved in distilbert-classification-uncased/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-classification-uncased/checkpoint-55000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12458\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84090, training_loss=0.015288903694017305, metrics={'train_runtime': 26905.3744, 'train_samples_per_second': 12.501, 'train_steps_per_second': 3.125, 'total_flos': 4.455593940754022e+16, 'train_loss': 0.015288903694017305, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train('distilbert-classification-uncased/checkpoint-30000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "771e1e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./distilbert-classification-uncased/checkpoint-80000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./distilbert-classification-uncased/checkpoint-80000\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./distilbert-classification-uncased/checkpoint-80000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./distilbert-classification-uncased/checkpoint-80000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = None\n",
    "trainer = None\n",
    "trained = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def softmax_outputs(outputs) -> dict:\n",
    "    return torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "\n",
    "trained = AutoModelForSequenceClassification.from_pretrained('./distilbert-classification-uncased/checkpoint-80000')\n",
    "trained.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4693df4a",
   "metadata": {},
   "source": [
    "# Run on full video transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9809776c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from data_loader import Caption, load_captions_from_chunks, segment_text\n",
    "\n",
    "def caption_times(c):\n",
    "    return c.start, c.end\n",
    "\n",
    "def prediction_times(p):\n",
    "    return tuple(p[0])\n",
    "\n",
    "def tumbling_time_window(captions, duration, key=caption_times):\n",
    "    results = [captions[0]]\n",
    "    for caption in captions:\n",
    "        if key(results[-1])[1] - key(results[0])[0] <= duration:\n",
    "            results.append(caption)\n",
    "        else:\n",
    "            yield results\n",
    "            results = [caption]\n",
    "\n",
    "    yield results\n",
    "    \n",
    "def session_time_window(captions, duration, key=caption_times):\n",
    "    captions_iter = iter(captions)\n",
    "    results = [next(captions_iter)]\n",
    "    for caption in captions_iter:\n",
    "        if key(results[-1])[1] - key(caption)[0] <= duration:\n",
    "            results.append(caption)\n",
    "        else:\n",
    "            yield results\n",
    "            results = [caption]\n",
    "\n",
    "    yield results\n",
    "\n",
    "def batch(iterable, n):\n",
    "    length = len(iterable)\n",
    "    for i in range(0, length, n):\n",
    "        yield iterable[i:min(i + n, length)]\n",
    "        \n",
    "def decode_label(outputs):\n",
    "    content, sponsor = outputs\n",
    "    \n",
    "    prediction_dict = {'sponsor': sponsor, 'content': content}\n",
    "    prediction_dict = {k: v for k, v in sorted(prediction_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return next(iter(prediction_dict.items()))\n",
    "        \n",
    "def predict_in_batches(texts, batch_size: int = 8):    \n",
    "    batches = list(batch(texts, batch_size))\n",
    "    for b in batches:\n",
    "        inputs = defaultdict(list)\n",
    "        for text in b:\n",
    "            tokenized = tokenize_function({ 'text': text })\n",
    "            for k, v in tokenized.items():\n",
    "                inputs[k].append(v)\n",
    "            \n",
    "        inputs = { k: torch.tensor(v).cuda() for k, v in inputs.items() }\n",
    "        outputs = trained(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1).tolist()\n",
    "        yield from predictions\n",
    "        \n",
    "def predict_sponsor_segments(captions, window_duration=10):\n",
    "    windows = list(tumbling_time_window(captions, window_duration))\n",
    "    window_texts = [segment_text(window) for window in windows]\n",
    "    predictions = predict_in_batches(window_texts, 4)\n",
    "    \n",
    "    for window, text, prediction in zip(windows, window_texts, predictions):\n",
    "        yield [window[0].start, window[-1].end], text, *decode_label(prediction)\n",
    "        \n",
    "def merge_prediction_(predictions):\n",
    "    assert len(set((label for _, _, label, _ in predictions))) == 1\n",
    "    # All co-occurring predictions have the same label so we merge them\n",
    "    merged_start, merged_end = predictions[0][0][0], predictions[-1][0][1]\n",
    "    merged_text = ' '.join((text for _, text, _, _ in predictions))\n",
    "    # Don't know what the correct way to compute the joint probability here is,\n",
    "    # just assume they are independent; We don't really use this number anywhere\n",
    "    prob = np.prod([prob for _, _, _, prob in predictions])\n",
    "    return [merged_start, merged_end], text, predictions[0][2], prob\n",
    "\n",
    "def merge_predictions(predictions, within_duration=5):\n",
    "    for co_occuring in session_time_window(predictions, within_duration, key=prediction_times):\n",
    "        merged = [co_occuring[0]]\n",
    "        for times, text, label, prob in co_occuring[1:]:\n",
    "            _, _, prev_label, _ = merged[0]\n",
    "            if label == prev_label:\n",
    "                merged.append((times, text, label, prob))\n",
    "            else:\n",
    "                yield merge_prediction_(merged)\n",
    "                merged = [(times, text, label, prob)]\n",
    "        \n",
    "        if len(merged) > 0:\n",
    "            yield merge_prediction_(merged)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cd0611cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mFound ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping --6T95cQa50 because sponsor times do not match the captions\n",
      "Dropping --BXjAWlPDQ because sponsor times do not match the captions\n",
      "Dropping --xfK_Uhly4 because sponsor times do not match the captions\n",
      "Dropping --yUuR_F_wU because sponsor times do not match the captions\n",
      "Dropping -1STsVEsLSU because sponsor times do not match the captions\n",
      "Dropping -2a7i00mcS0 because sponsor times do not match the captions\n",
      "Dropping -3bMKfaMY7I because sponsor times do not match the captions\n",
      "Dropping -3GY3WjZY4Y because sponsor times do not match the captions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055192f15de04fdc826accb9c5a0251f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n",
      "\u001b[45m-3nw9slXrBc [(979.606, 1044.58)]\u001b[0m\n",
      "\tPredicted=[('99%', [979.606, 1022.91]), ('99%', [1035.2, 1059.683]), ('99%', [1313.259, 1324.0])],\n",
      "\tExpected=[(979.606, 1044.58)]\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def evaluate(videos):\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    predicted = []\n",
    "    expected = []\n",
    "    \n",
    "    output = []\n",
    "    for video_id, captions, sponsor_ranges in tqdm(videos):\n",
    "        sponsor_times = [(captions[start].start, captions[end].end) for start, end in sponsor_ranges]\n",
    "\n",
    "        output.append(lambda: print(colored(f'{video_id} {sponsor_times}', None, 'on_magenta')))\n",
    "        predicted_sponsor_times = []\n",
    "\n",
    "        for times, text, label, prob in merge_predictions(predict_sponsor_segments(captions, window_duration=10), within_duration=10):\n",
    "            if label == 'sponsor':\n",
    "                predicted_sponsor_times.append((f'{int(prob * 100)}%', times))\n",
    "\n",
    "            color = { 'sponsor': 'yellow', 'content': None }[label]\n",
    "            # print(colored(f'{int(prob * 100)}% {times[0]} <--> {times[1]} {text}', color=color))\n",
    "\n",
    "        predicted.append(predicted_sponsor_times)\n",
    "        expected.append(sponsor_times)\n",
    "        output.append(lambda: print(f'\\tPredicted={predicted_sponsor_times},\\n\\tExpected={sponsor_times}'))    \n",
    "    \n",
    "    # TODO: Evaluate predicted vs. expected\n",
    "    \n",
    "    for o in output:\n",
    "        o()\n",
    "        \n",
    "evaluate(list(itertools.islice(load_captions_from_chunks('data', './', [1]), 0, 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ce27f",
   "metadata": {},
   "source": [
    "# Experiment with Topical Change Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12cdce02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c0d21c95fe4b7889f3494174e210c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd390fbe17a04f409d921a1828e9410a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531b9188c0de48d98c35b4851e169df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0190dad9794956880344c062904a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6c1a3618d6428b9178b30da70bfa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34bb736f72149dbb37354c875746c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dennlinger/roberta-cls-consec were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tc_tokenizer = AutoTokenizer.from_pretrained('dennlinger/roberta-cls-consec')\n",
    "tc_model = AutoModelForSequenceClassification.from_pretrained('dennlinger/roberta-cls-consec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "972561d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not_same': '0.00', 'same': '1.00'} oh my god  i'm holding my life back in my hands\n",
      " right now it feels like this is the only\n",
      "\n",
      "{'not_same': '0.21', 'same': '0.79'} good thing that being severely anemic\n",
      "has ever done for me [Music]  okay wait actually before we go to\n",
      "\n",
      "{'not_same': '0.28', 'same': '0.72'} boston i want to tell you guys about the\n",
      " sponsor today's video which is current i\n",
      " mean it's a way that i'm paying for this\n",
      " entire trip in general so it might as\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} well tell you about how i'm doing this\n",
      " in the first place\n",
      " while wearing their awesome hat that\n",
      " they sent me as well current is the new\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} way to bank it is truly the future i\n",
      " mean i feel like we're all always\n",
      " dreaming of what the future holds and\n",
      " honestly it is you with this car\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} current is a mobile bank with a visa\n",
      " debit card and a real bank account with\n",
      " no hidden fees and no minimum balance\n",
      " requirement current gets you paid\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} actually up to two days faster than\n",
      " regular cards aka if you normally get\n",
      " paid on fridays with this card you're\n",
      " actually gonna get paid on wednesdays i\n",
      "\n",
      "{'not_same': '0.16', 'same': '0.84'} don't know why you wouldn't want that\n",
      " and with current you also have the\n",
      " ability to deposit checks from the\n",
      " camera on your phone into your bank\n",
      "\n",
      "{'not_same': '0.09', 'same': '0.91'} account they also have a free overdraft\n",
      " fee for up to a hundred dollars which is\n",
      "huge huge\n",
      " and you guys can even earn points from\n",
      "\n",
      "{'not_same': '0.10', 'same': '0.90'} your purchases and redeem them for cash\n",
      " and current is the only place where that\n",
      " is happening other cool things about\n",
      " current is that they have over 55 000\n",
      "\n",
      "{'not_same': '0.06', 'same': '0.94'} free atms for you to use they give you\n",
      " instant refunds on gas station holes\n",
      " they work with all the major money\n",
      " transferring services like google pay\n",
      "\n",
      "{'not_same': '0.21', 'same': '0.79'} apple pay venmo\n",
      " and the best part about all that is that\n",
      " it comes with no fees for transferring\n",
      " any of your money yeah that's right not\n",
      "\n",
      "{'not_same': '0.70', 'same': '0.30'} only do they want you to get your money\n",
      " faster each week but they also are not\n",
      " taking it with a ton of stupid fees for\n",
      " a million different things because\n",
      "\n",
      "{'not_same': '0.53', 'same': '0.47'} that's just not them and that's not what\n",
      " they want to do it takes two minutes to\n",
      " sign up for current and then your card\n",
      " will ship for free right to your door so\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} i don't know what you're waiting for\n",
      " click the link in my description right\n",
      " now and get your current card right now\n",
      " and let's get spending\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} and now without further ado we're gonna\n",
      " go to boston so let's\n",
      "[Music] up go the morning i can't just let you [Applause]\n",
      "{'not_same': '0.01', 'same': '0.99'} [Applause]\n",
      "[Music] go go\n",
      " okay [ __ ] here i am i am home\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} and i'm so happy to be here i feel like\n",
      " i have not been home in forever and the\n",
      " fact that when i came home for christmas\n",
      " you guys obviously didn't get a vlog\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} which means i haven't vlogged here\n",
      " since last october is crazy to me time\n",
      " is flying by and i'm so confused\n",
      " me rory and johnny all decided to come\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} home to boston this week just to get a\n",
      " little bit of a break\n",
      " and see our families but another reason\n",
      " why i wanted to come home\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} is because boston announced that all of\n",
      "the the\n",
      " arenas and sports and concert venues are\n",
      " opening back up for sports games at a\n",
      "\n",
      "{'not_same': '0.19', 'same': '0.81'} very limited capacity and i wanted to\n",
      " try and come and go to one because at\n",
      " this point i don't care what\n",
      " entertainment i see live i need\n",
      "\n",
      "{'not_same': '0.08', 'same': '0.92'} something in front of me physically\n",
      " entertaining me in an arena before i\n",
      " lose my goddamn mind\n",
      " and that's what i'm doing today i'm\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} going to a bruins game with rory and my\n",
      " dad aiden but i'm really excited i don't\n",
      " even think i've ever been to a bruins\n",
      "game game\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} in boston before which is so lame\n",
      " because i'm a football girly i like\n",
      " football i got the patriot sweatshirt on\n",
      " you can hardly see the steering wheel is\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} seriously just in the way but i'm\n",
      " getting starbucks right now\n",
      " it is 3 48. i need to leave at five and\n",
      " i need to just pull my life together oh\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} my god i almost just laid on the horn\n",
      " [ __ ] me laying on the horn in the\n",
      " starbucks line right now for no reason\n",
      " yeah i can't believe i get to go home\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} right now and get ready to go to an\n",
      " arena and go see something live\n",
      " i'm just really happy that things are\n",
      " starting to pick back up and get normal\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} feeling [ __ ] good all right i just\n",
      " got my drink i got\n",
      " a grande vanilla sweet cream cold brew\n",
      " with white mocha and caramel syrup yes\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} so much sugar and crap in it but\n",
      " whatever i do not like the taste of\n",
      " coffee at all so i like to drown it out\n",
      "with with\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} a million different things this is\n",
      " absolutely hitting the spot this is one\n",
      " of the first vlogs in a very long time\n",
      " that i actually feel good about before i\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} even pick up my camera and i just know\n",
      " what i'm gonna\n",
      " do and i know i'm gonna have fun and i\n",
      " know it's gonna be fun to edit and i\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} know you guys are gonna like it so\n",
      " i am excited i need to go home now\n",
      " though because i have an hour to get\n",
      " ready and i have a cute yellow black and\n",
      "\n",
      "{'not_same': '0.18', 'same': '0.82'} white outfit picked out for the bruins\n",
      "so so\n",
      " let's [ __ ] go\n",
      "[Music] [Music]\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} all right aidan how are you feeling very\n",
      " good exciting\n",
      "great answer thank you  all right aidan give me your post game\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} interview how do you feel\n",
      " we lost i'm not good how do you feel i\n",
      " didn't even know\n",
      " overtime started yet i was still looking\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} at my phone i looked up and we lost\n",
      " hey besties it's like 1 15 in the\n",
      "afternoon afternoon\n",
      " the next day me and rory stayed at a\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} hotel right across the street from td\n",
      "garden garden\n",
      " yesterday was wild and inspiring i\n",
      " really still\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} cannot process the fact that i was just\n",
      " in a crowd\n",
      " of people at an arena again like i just\n",
      " i'm still like what the [ __ ] was that\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} was the past year blur\n",
      " and just seeing the arena at only 12 i\n",
      " was just like what the [ __ ] is going on\n",
      " but that was so\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} liberating and i cannot wait for you all\n",
      " to experience that too like if sports\n",
      " games are opening back up where you live\n",
      "just just\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} just go seriously just blow your [ __ ]\n",
      " money and go to one just to be in a\n",
      " crowd like that i'm telling you it's\n",
      " worth every cent i'm feeling amazing\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} thriving truly it's friday right now and\n",
      " i leave monday morning and i'm already\n",
      " just like this trip was way too short i\n",
      " need this to be way\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} longer i'm just so happy to be home and\n",
      " the weather's been nice and everything\n",
      " is just getting better and i'm just\n",
      " feeling optimistic and that's all i can\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} ask for\n",
      " i just can't stop thinking about this\n",
      " one moment last night where the burns\n",
      " got a gold just the whole arena stood up\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} at the same time and started screaming\n",
      " and i was just like oh oh wow like a\n",
      " huge serotonin rush\n",
      " right then and there to charge me\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} serotonin for the next month i don't\n",
      " know what i just said but i said it so\n",
      " we're gonna move on me rory and his dad\n",
      " also went out last night and had some\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} drinks so that was wholesome that was\n",
      "fun fun\n",
      " it just felt normal so i really\n",
      " appreciated that i had so much fun with\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not_same': '0.00', 'same': '1.00'} my dad and aiden but now tonight is\n",
      " about my mom\n",
      " i'm going out with her and her friends\n",
      " the ladies are going to go get some\n",
      "drinks\n",
      "{'not_same': '0.01', 'same': '0.99'} drinks\n",
      " let me show you the view of the hotel\n",
      " room right now i'm sure you really care\n",
      " but i personally appreciate it so i want\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} you guys to appreciate with me we've got\n",
      " the highway we've got the garden right\n",
      " there oh yeah i lost a nail last night\n",
      " so that's really sad oh my god my nails\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} so gross actually don't look at it it's\n",
      " kind of gross out kind of annoyed about\n",
      "that that\n",
      " but yeah awesome 10 out of 10 scenery\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} probably talk to you guys tomorrow\n",
      " because i'm really [ __ ] excited about\n",
      " that that's what i'm going to see\n",
      " all of my friends and go out to eat and\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} get a hotel and go shopping and\n",
      " everything so let's just [ __ ] cut to\n",
      " that honestly like let's get to the\n",
      " point let's see my friends again all\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} right here we go\n",
      " like bring it on um if you can't tell\n",
      " the three of us are back and better\n",
      " we're literally here to cause some\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} issues we just picked james up at\n",
      "lululemon lululemon\n",
      " still there nothing changed so once\n",
      " again like i say every single time james\n",
      "\n",
      "{'not_same': '0.06', 'same': '0.94'} works at lululemon please don't forget\n",
      " it please go harass him\n",
      " if you live in the boston area they do\n",
      " yes oh yes okay so everything is normal\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} you guys are doing your job thank you so\n",
      " much oh really cute dog ahead of me\n",
      " yeah it's going to bathroom we're\n",
      " currently going on a shopping spree\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} where are we going first\n",
      " brandy because kayla's having me we love\n",
      " that live oh i'm having a meet and greet\n",
      " no in the most literal sense two weeks\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} ago i bought every single color\n",
      " zip up that brandy has now i have six of\n",
      " them i also have some incredible news we\n",
      " might be making a comeback of mommy made\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} a mistake\n",
      " might do it today mommy will no we are\n",
      "doing  okay he's making a terrible amazing\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} mistake unclear what the mistake will be\n",
      " exactly but it is happening\n",
      " and it will be very expensive but my tax\n",
      " guy said this morning i can do it so i'm\n",
      "\n",
      "{'not_same': '0.04', 'same': '0.96'} gonna do it\n",
      " yeah this one hurt you too much jason\n",
      " said go for it so i'm gonna go for it we\n",
      " look really good yeah we look really\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} good okay i'd be scared if i wasn't\n",
      " honest oh four five i hate that oh my\n",
      " god this line is going to take forever\n",
      " so i go to the front of the line and go\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} hey i am i know that girl brandi can we\n",
      " just get in there\n",
      " she told me to like text you guys when i\n",
      " was here yeah i was like hey just go to\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} the front of the line and say hey we're\n",
      " with brandi\n",
      " no i really think so really sad update\n",
      " we did not go to brandy melville the\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} line was just obnoxiously long now we\n",
      " are going to get a juice because\n",
      " have to eat juice nope drink juice\n",
      " strangers and\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} so yeah we're doing that and i'm going\n",
      " to go make a mistake somewhere so we'll\n",
      " keep you updated on the place that i\n",
      " choose to make mistake at i almost just\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} ran into a poll\n",
      " james where are you going jc smoothie\n",
      " are you coming with me oh we've never\n",
      " been there before but we must have run\n",
      "hey guys let's go\n",
      "{'not_same': '0.01', 'same': '0.99'}  what do they say oh my god here we go\n",
      " not another shot\n",
      " what how was that one going crazy today\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} it tastes like strawberries and bananas\n",
      " that's what it is well considering it's\n",
      "a smoothie i'm gonna have to agree  okay i'm doing something bad we'll keep\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} you updated there's a mistake being made\n",
      " as we speak i don't know if it's a\n",
      " mistake though actually it was meant to\n",
      "happen happen\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} fight maybe before dude she got a\n",
      "manicure manicure\n",
      "how about that how are we gonna film  oh no oh no oh no okay one mistake down\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} might have one more to go\n",
      " one more i might i don't know i'm\n",
      " feeling crazy thank you karen for\n",
      "sponsoring this video \n",
      "{'not_same': '0.03', 'same': '0.97'} what am i doing i haven't made a mistake\n",
      " in years and i'm doing it right now\n",
      " and one of the three little digits on\n",
      " the back of your card what are those oh\n",
      "\n",
      "{'not_same': '0.43', 'same': '0.57'} let's shut up\n",
      " shut up right now you know what the guy\n",
      " who does my taxes said i can do this\n",
      " this morning so three\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} that's great what did i just do i'm\n",
      " gonna pretend that the receipt is an\n",
      " error and that the decimal place is\n",
      " accidentally moved over like right\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} and it was just 22. yeah yeah oh oh miss\n",
      " girl ole miss girl\n",
      " mommy tell us what you did we went up\n",
      " from the last mistake that i made and\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} today i made two mistakes at once\n",
      " and now i'm going to a steakhouse i'm\n",
      " also going to steakhouse i'll be there\n",
      " what are you eating maybe salmon i'm\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} going to go throw up now\n",
      " okay great i'll come with you cool okay\n",
      " let's go come on now with my throat\n",
      " let's go okay so we just checked into\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} our room and there's blood all over the\n",
      " curtain we've now been upgraded because\n",
      " we don't want a\n",
      " room with um you know this mystery\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} substance that i'm going to assume is\n",
      " blood she's constantly raising the bar\n",
      " no now we're raising the floor level so\n",
      " here we go unfortunately we have to say\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} goodbye to our gorgeous view\n",
      " oh no not this okay guys i'm ready to\n",
      " start my haul but i need my lovely\n",
      " assistant james here\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} hi here we go welcome back to a caitlyn\n",
      " james hall not like we've ever done this\n",
      "before before\n",
      " great so welcome to the first yes the\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} first of many we're planning on having a\n",
      " crazy year crazy summer crazy life okay\n",
      " great i like that you're introduced\n",
      " yeah i'm setting us up for more failures\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} like this in the future right of course\n",
      " um okay so as we all know mommy made a\n",
      " mistake has come back today\n",
      " and i'd like to present to you two\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} mistakes that have been made this is\n",
      " very unlike me\n",
      " i bought one thing that i have been\n",
      " begging for for years from myself and\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} myself finally let myself have it so it\n",
      " was a moment but first drum roll here we\n",
      "go  look at the packaging why do i have this\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} in my hands in general why did i buy\n",
      "this this\n",
      " yeah that's the receipt okay so we're\n",
      " totally gonna put the receipt in the\n",
      "trash\n",
      "{'not_same': '0.01', 'same': '0.99'} trash\n",
      " no kidding don't um there are shoes that\n",
      " have gone viral from gucci and i wanted\n",
      " them last fall\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} then i decided it was the fall like\n",
      " there's no need to get them because it's\n",
      " going to be the fall\n",
      " so instead i decided to get them now i\n",
      "\n",
      "{'not_same': '0.80', 'same': '0.20'} was originally planning on getting a\n",
      " paint color but i was talked out of it\n",
      " by my good friend james and my good\n",
      " friend lindsay i now have gucci\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} jelly flip flops aren't they so cute you\n",
      " guys it's like a fun\n",
      " idea ivory i feel like i'm not gonna\n",
      " take these off ever 24 7. i mean as we\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} all know i've been in\n",
      " unfortunately multiple paparazzi\n",
      " pictures of me with uggs on oh\n",
      " because i don't really have any other\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} like flip-flops or sandals right\n",
      " so i'm hoping that this is the upgrade\n",
      " that i need i think it's the upgrade\n",
      " that you need in dessert but for some\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} reason i know i'm gonna end up wearing\n",
      " my eggs anyways but we can now hold me\n",
      " accountable because i have these don't\n",
      " deserve them should have gotten a\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not_same': '0.00', 'same': '1.00'} manicure first\n",
      " i was just about to here they are\n",
      " the next step manicure yes let's just\n",
      " get on that they only would let me have\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} one shoe again\n",
      " the other one's in a bag but i just\n",
      " don't feel like unboxing that one so\n",
      " that's what i have ready to take on the\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} summer with these shoes\n",
      " i'm destroying the box with water really\n",
      " good inspirational things oh\n",
      "wait look at the real prize \n",
      "{'not_same': '0.01', 'same': '0.99'} are you kidding me oh wow i have to pull\n",
      " myself together and you did buy\n",
      " something else\n",
      " i did buy something else i bitched about\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} this item in this bag for years\n",
      " yes now i do not buy designer bags often\n",
      " i truly only bought one which is my\n",
      " iconic balenciaga that i bought in\n",
      "hawaii\n",
      "{'not_same': '0.05', 'same': '0.95'} hawaii\n",
      " fall 2019. which one are you wearing\n",
      " tonight i think this\n",
      " this bag has been sold out for a very\n",
      "\n",
      "{'not_same': '0.40', 'same': '0.60'} long time which is probably been the\n",
      " best for me\n",
      " today we had the unfortunate\n",
      " circumstances of the bag coming back in\n",
      "stock\n",
      "{'not_same': '0.00', 'same': '1.00'} stock\n",
      " at one location in boston and um guess\n",
      " how many you came back and stopped did\n",
      " you guys guess one because if you did\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} you were right yeah just one\n",
      " a single one and i i have it and i was\n",
      " kind of bullied i wasn't bullied into\n",
      " getting it come on there was a bully it\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} was like caitlyn you should get it and\n",
      " she was like you're right i'm gonna get\n",
      "it it\n",
      " the only reason why i didn't black out\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} at the cash register is because i think\n",
      " about this bag 24 7 and if i didn't get\n",
      " it now i would have got it at another\n",
      " point in time because this is the only\n",
      "\n",
      "{'not_same': '0.03', 'same': '0.97'} bag i think about because because it's\n",
      "gorgina gorgina\n",
      " that's james's favorite word for those\n",
      " of you who don't know gorgeous vagina oh\n",
      "wow\n",
      "{'not_same': '0.02', 'same': '0.98'}  we're going to give more of a reaction\n",
      " to the dust bag of the actual bag\n",
      " everyone give it up for her\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} here she is ladies and gentlemen yes so\n",
      " this is my new backpack it fits my\n",
      " camera which is why i bought it and\n",
      " which is why i think it is perfect for\n",
      "me\n",
      "{'not_same': '0.01', 'same': '0.99'} me\n",
      " and now i have an everyday bag that is\n",
      " nice and i can't believe i bought this\n",
      " for myself and i'm confused\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} you deserve miss girl i worked hard for\n",
      " this bag i'm really proud of myself oh\n",
      " i wanted this for years oh no\n",
      " not the bad falling in the trash\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} literally just launched her back into\n",
      " the trash her absolutely\n",
      " i just threw my new bag in the trash my\n",
      " battery just died my bag went in the\n",
      "\n",
      "{'not_same': '0.06', 'same': '0.94'} trash everything's normal it is 6 30\n",
      " right now\n",
      " we are getting ready to go on an\n",
      " adventure have a fabulous night that we\n",
      "are\n",
      "{'not_same': '0.08', 'same': '0.92'} are\n",
      " a night that we deserve i have to break\n",
      " my bag out in ways that aren't throwing\n",
      " it in the trash\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} yeah i'm going to change now okay ready\n",
      " here we go i'm going to put this on\n",
      " including my fake boobs one two three\n",
      " here we go\n",
      "here we are give it up for us\n",
      "{'not_same': '0.01', 'same': '0.99'}  me putting applause in the background\n",
      " without further ado we're now going to\n",
      "go to the palm and black out right mike \n",
      "{'not_same': '0.01', 'same': '0.99'} all right mike what are you getting at\n",
      " the palm tonight the most expensive\n",
      " steak on the menu\n",
      " and then complain about it later i'm\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} gonna drink before i order the steak and\n",
      "then then\n",
      " i'm gonna complain about it later okay\n",
      " perfect i'm ready to get drunk and have\n",
      "fun so here i go\n",
      "{'not_same': '0.02', 'same': '0.98'}  so in typical beyonce fashion formation\n",
      " we're getting information and we are\n",
      " going to star market\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} in less than four minutes because we\n",
      " would like white claws and they close\n",
      " in five minutes so here we go we don't\n",
      " feel like drinking the\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} boys um whatever it is\n",
      " yeah like captain morgan i'm literally\n",
      " all [ __ ] set so here we go on our\n",
      " adventure to get um white clothes less\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} than five minutes so here we go we can\n",
      " do it we can\n",
      "[Music] [Music]\n",
      " we really said [ __ ] four minutes we'll\n",
      "get there in one second\n",
      "{'not_same': '0.01', 'same': '0.99'} [Applause] [Applause]\n",
      "[Music] [Music]\n",
      " so somehow this week is already over i\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} literally do not know how um\n",
      " yes i have zit stickers on my face right\n",
      " now i have a big long one here and\n",
      " another big long one here so just\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} bear with me but like it's already time\n",
      " for me to go home right about it all\n",
      " last night i had\n",
      " the best time with my friends it's like\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} not even funny i needed that\n",
      " so bad and coming home is just so fun\n",
      " now and the fact that everyone just\n",
      " got to be all together last night just\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} made me so [ __ ] happy i wanted to\n",
      " change my flight so bad and stay here\n",
      " longer but like i\n",
      " can't that sucks but yeah i'm sitting in\n",
      "\n",
      "{'not_same': '0.33', 'same': '0.67'} aiden's old room now which is now my\n",
      " room when i come home\n",
      " here is a tour it's literally so tiny\n",
      " yes these are stairs for many to go up\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} the bed cannot believe i'm stepping into\n",
      " lax like this what a tragedy maybe i'll\n",
      " rip all my nails off on the flight i\n",
      " would really like to show you the bed\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} sheets i'm sleeping on right now though\n",
      " they're kind of like my childhood bed\n",
      " sheets but i kind of forgot what they\n",
      " said on them but\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} take a good look at these these are\n",
      " literally hawaii bed sheets\n",
      " that i forgot that i slept on my entire\n",
      " childhood like no [ __ ] [ __ ] i ended\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} up moving to hawaii and no wonder i'm\n",
      " just absolutely obsessed with it like\n",
      " these were the bed sheets that i slept\n",
      " on all the time and growing up i'd never\n",
      "\n",
      "{'not_same': '0.05', 'same': '0.95'} even been to hawaii so i don't know why\n",
      " my mom chose these but she did\n",
      " and um this is me saying that like\n",
      " everything happens for a reason all\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} right i'm gonna go now that makes me so\n",
      " sad i don't want to be ending this vlog\n",
      " at all i like hate that i didn't even\n",
      " get to like do anything i had to wake up\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} from the hotel\n",
      " and go right home and return my rental\n",
      " car and i didn't get to go to like\n",
      " brunch with all my friends i barely even\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} got to say bye to everyone well i said\n",
      " bye to everyone last night but like i\n",
      " didn't get to do anything fun today and\n",
      " now i'm just like sad that this is how\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} my video is ending like it's just over\n",
      " now but that's okay\n",
      " because i'll be back wait also don't\n",
      " forget to click the link in my bio to\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} get your current card today thank you\n",
      " kern for sponsoring i love you\n",
      " can't wait to see what mistake i make\n",
      " with this card next time will only tell\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} i will see you guys in my next video i\n",
      " hope you [ __ ] love this like i did\n",
      " thank you for everything\n",
      " so crazy how my life is now compared to\n",
      "\n",
      "{'not_same': '0.00', 'same': '1.00'} how it was living here and growing up\n",
      "here here\n",
      " i'm just so [ __ ] grateful like thank\n",
      " you guys for everything in this life\n",
      "like\n",
      "{'not_same': '0.00', 'same': '1.00'} like\n",
      " seriously i do not know how i got so\n",
      " lucky and coming home just makes me feel\n",
      " like so connected with that too\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} especially when i go out in boston and\n",
      " just meet so many people who watch my\n",
      " videos the second i get out of my car\n",
      " like i don't even know how that's a\n",
      "\n",
      "{'not_same': '0.02', 'same': '0.98'} thing either i met so many people\n",
      " yesterday it's crazy even when i was\n",
      " driving i had girls\n",
      " wave at me from their cars so like i\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not_same': '0.01', 'same': '0.99'} don't know it just feels really good to\n",
      " just like\n",
      " reconnect and just remember where i came\n",
      " from i don't know\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} thank you guys for [ __ ] everything\n",
      " and i will see you in my next\n",
      " video april like kind of might be fun\n",
      " like i don't know like we'll see i have\n",
      "\n",
      "{'not_same': '0.31', 'same': '0.69'} some things planned that i think you\n",
      " guys will like but\n",
      " we'll [ __ ] see no more peace signs i\n",
      " need to end this video i love you guys\n",
      "\n",
      "{'not_same': '0.01', 'same': '0.99'} so much oh my god i went to a sports\n",
      " game in this video holy [ __ ]\n",
      "[ __ ]  bring that freaks out i put choices\n",
      "through\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mzip_longest(fillvalue\u001b[38;5;241m=\u001b[39mfillvalue, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m left_captions, right_captions \u001b[38;5;129;01min\u001b[39;00m group(group(videos[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m2\u001b[39m, []):\n\u001b[0;32m----> 6\u001b[0m     left_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mleft_captions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     right_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((caption[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m caption \u001b[38;5;129;01min\u001b[39;00m right_captions))\n\u001b[1;32m      8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tc_tokenizer(left_text, right_text)\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mzip_longest(fillvalue\u001b[38;5;241m=\u001b[39mfillvalue, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m left_captions, right_captions \u001b[38;5;129;01min\u001b[39;00m group(group(videos[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m2\u001b[39m, []):\n\u001b[0;32m----> 6\u001b[0m     left_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((\u001b[43mcaption\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m caption \u001b[38;5;129;01min\u001b[39;00m left_captions))\n\u001b[1;32m      7\u001b[0m     right_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin((caption[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m caption \u001b[38;5;129;01min\u001b[39;00m right_captions))\n\u001b[1;32m      8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tc_tokenizer(left_text, right_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for left_captions, right_captions in group(group(videos[2][1], 2), 2, []):\n",
    "    left_text = ' '.join((caption['text'] for caption in left_captions))\n",
    "    right_text = ' '.join((caption['text'] for caption in right_captions))\n",
    "    inputs = tc_tokenizer(left_text, right_text)\n",
    "    inputs = { k: torch.tensor([v]) for k, v in inputs.items() }\n",
    "    outputs = tc_model(**inputs)\n",
    "    non_sponsor, sponsor = torch.nn.functional.softmax(outputs.logits, dim=-1).tolist()[0]\n",
    "    \n",
    "    print({ 'not_same': '%.2f' % non_sponsor, 'same': '%.2f' % sponsor }, left_text, right_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e751ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9dc7759ca38c0da6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/veselin/.cache/huggingface/datasets/json/default-9dc7759ca38c0da6/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771a0ba6f905432fb5f7b5356fecdda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4677dd94698144b39594e56e11e74ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read file '/home/veselin/.cache/huggingface/datasets/downloads/extracted/58b78e85c06cba74c6ee3cc9f04a35a99e1d7a2d5eb255e822ce8bc17b8ba33d' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/captions/[]/[]) changed from string to number in row 0\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "JSON parse error: Column(/captions/[]/[]) changed from string to number in row 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py:144\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 144\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03ma JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03mkwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtra data\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 12532)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.1.json.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/load.py:1691\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1691\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_verifications\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_verifications\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1701\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   1702\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/builder.py:605\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF google storage unreachable. Downloading and preparing it from source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_infos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/builder.py:694\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m    701\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/builder.py:1151\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator)\u001b[0m\n\u001b[1;32m   1149\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_tables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generator\u001b[38;5;241m.\u001b[39mgen_kwargs)\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ArrowWriter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures, path\u001b[38;5;241m=\u001b[39mfpath) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, table \u001b[38;5;129;01min\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   1152\u001b[0m         generator, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m tables\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# not logging.is_progress_bar_enabled()\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m     ):\n\u001b[1;32m   1154\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(table)\n\u001b[1;32m   1155\u001b[0m     num_examples, num_bytes \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/tqdm/std.py:1183\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py:146\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    144\u001b[0m             dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot able to read records in the JSON file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should probably indicate the field of the JSON file containing your records. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis JSON file contain the following fields: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect the correct one and provide it as `field=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXXX\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to the dataset loading method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Uncomment for debugging (will print the Arrow table size and elements)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py:122\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m         pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mpaj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReadOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid, pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/pyarrow/_json.pyx:259\u001b[0m, in \u001b[0;36mpyarrow._json.read_json\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/NLU-S1NbPIk9/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: JSON parse error: Column(/captions/[]/[]) changed from string to number in row 0"
     ]
    }
   ],
   "source": [
    "load_dataset('json', data_files='data.1.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0e95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
