{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee584dd0",
   "metadata": {},
   "source": [
    "# Sponsor content detection in YouTube videos\n",
    "## Transfomers for binary text classification\n",
    "This notebook seeks to accomplish the task of sponsored-content detection using a binary text classification model. The text classification model is created by fine-tuning a DistilBERT pre-trained model.\n",
    "\n",
    "## Motivation\n",
    "Several similar projects based on a BERT-type text classification model have been written about in on the Internet. Unfortunately, in both instances the authors do not share details about the performance of the model. Instead, they used vague language like \"95% accuracy\" without qualifying that in any meaningful way. What is more, the trained models in both instances then demonstrably perform poorly in the downstream task of task classification, but no exact numbers are reported. \n",
    "\n",
    "We wanted to investigate how well a text classification model can perform on what is essentially a span extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da052c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, IterableDataset, IterableDatasetDict, ClassLabel, load_dataset, load_from_disk, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.realpath('..')))\n",
    "from data_loader import load_examples_from_chunks, load_captions_from_chunks, get_intersection_range, segment_text, load_data_from_chunks\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421da5b",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "Read the transcripts from the `data.N.json.gz` and extract examples using `load_examples_from_chunks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_video(captions, segment_ranges):\n",
    "    \"\"\"\n",
    "    Convert a list of captions with multiple sponsor ranges\n",
    "    into multiple lists of captions with 0-1 sponsor ranges each.\n",
    "    \"\"\"\n",
    "    if len(segment_ranges) == 1:\n",
    "        return captions, segment_ranges[0]\n",
    "    \n",
    "    segment_ranges.sort()\n",
    "    \n",
    "    last_chunk_end = 0\n",
    "    for i, r in enumerate(segment_ranges):\n",
    "        start_idx, end_idx = r\n",
    "        if i + 1 < len(segment_ranges):\n",
    "            # Pick the mid-point between the end of this segment and the start of the next\n",
    "            # to end this chunk\n",
    "            chunk_end = (end_idx + segment_ranges[i + 1][0]) // 2\n",
    "        else:\n",
    "            chunk_end = len(captions) - 1\n",
    "        \n",
    "        yield captions[last_chunk_end:chunk_end], start_idx - last_chunk_end, end_idx - last_chunk_end\n",
    "        last_chunk_end = chunk_end\n",
    "\n",
    "MAX_DURATION_PER_TOKEN = 1\n",
    "        \n",
    "class SpanExtractionDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset: 'Iterable'):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for video_id, captions, sponsor_times in self.dataset:\n",
    "            drop_row = False\n",
    "            segment_ranges = [get_intersection_range(captions, start_time, end_time) for start_time, end_time in sponsor_times]\n",
    "            # Filter out broken ranges\n",
    "            segment_ranges = [r for r in segment_ranges if r[0] is not None and r[1] is not None and r[0] != r[1]]\n",
    "\n",
    "            for caption_chunk, start_idx, end_idx in chunk_video(captions, segment_ranges):\n",
    "                text = segment_text(caption_chunk)\n",
    "                sponsor_text = segment_text(caption_chunk[start_idx:end_idx + 1]).strip()\n",
    "                if len(sponsor_text) == 0:\n",
    "                    continue\n",
    "                \n",
    "                start_char_idx = text.index(sponsor_text)\n",
    "                end_char_idx = start_char_idx + len(sponsor_text)\n",
    "                yield text, start_char_idx, end_char_idx\n",
    "                \n",
    "# Chunks 1-15 for training but skip 12 because it has a broken encoding (not UTF-8)\n",
    "raw_train_dataset = SpanExtractionDataset(load_data_from_chunks('data', './', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]))\n",
    "raw_test_dataset = SpanExtractionDataset(itertools.islice(load_data_from_chunks('data', './', [16]), 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0900e",
   "metadata": {},
   "source": [
    "# Tokenize inputs\n",
    "Tokenize the datatset with the pre-trained tokenizer. Sequences are padded to the maximum length supported by BERT and truncated if longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "922f5381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def index_of_token(offset_mapping, char_index, default_value):\n",
    "    for i, r in enumerate(offset_mapping):\n",
    "        if i == 0:\n",
    "            # Skip the [CLS]\n",
    "            continue\n",
    "        if r[0] <= char_index <= r[1]:\n",
    "            return i\n",
    "        \n",
    "    return default_value\n",
    "\n",
    "class TokenizedSpanExtractionDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, dataset: 'Iterable', tokenize_function):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenize_function\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for text, start_char_idx, end_char_idx in self.dataset:\n",
    "            output = self.tokenizer(text)\n",
    "            \n",
    "            offset_mapping = output['offset_mapping']\n",
    "            # 0 is the special ignored_index [CLS]\n",
    "            start_position = index_of_token(offset_mapping, start_char_idx, default_value=0)\n",
    "            end_position = index_of_token(offset_mapping, end_char_idx, default_value=0)\n",
    "            if start_position != 0 and end_position == 0:\n",
    "                end_position = len(output['input_ids']) - 2\n",
    "            \n",
    "            yield {**output, 'start_char_index': start_char_idx, 'end_char_index': end_char_idx, 'start_positions': start_position, 'end_positions': end_position}\n",
    "\n",
    "            \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding='max_length',\n",
    "        stride=128,\n",
    "    )\n",
    "\n",
    "train_dataset = TokenizedSpanExtractionDataset(raw_train_dataset, tokenize_function)\n",
    "test_dataset = TokenizedSpanExtractionDataset(raw_test_dataset, tokenize_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137e60e8",
   "metadata": {},
   "source": [
    "# Prepare for training\n",
    "Set training parameters, configure metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "370ca77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.19.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert-span-extraction-uncased\", \n",
    "    per_device_train_batch_size=48, \n",
    "    per_device_eval_batch_size=48,\n",
    "    save_total_limit=2, \n",
    "    max_steps=9_000,\n",
    "    save_steps=300,\n",
    "    eval_steps=301,\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    ignore_data_skip=True)\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5300a6d",
   "metadata": {},
   "source": [
    "# Train the model âš¡\n",
    "We're using the default number of batches, but we terminate the training early because we observe that the model performs extremely well on all metric on the test dataset and because the training loss and validation loss are comparable after step 30,000, indicating that there is not too much over- or under-fitting, and that the model is not likely to learn anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2d32fe5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./distilbert-span-extraction-uncased/checkpoint-2100).\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 432000\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 48\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9000\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 2100\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForQuestionAnswering.forward` and have been ignored: start_char_index, end_char_index, offset_mapping. If start_char_index, end_char_index, offset_mapping are not expected by `DistilBertForQuestionAnswering.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 2:37:31, Epoch 18/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2107</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2408</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2709</td>\n",
       "      <td>0.595200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3311</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3612</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3913</td>\n",
       "      <td>0.282900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4214</td>\n",
       "      <td>0.209500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4515</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4816</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5117</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5418</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5719</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6321</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6622</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6923</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7224</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7525</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7826</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8127</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8428</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8729</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-2400\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-2400/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-2400/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-2700\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-2700/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-2700/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-3000\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-3000/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-2400] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-3300\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-3300/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-3300/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-2700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-3600\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-3600/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-3600/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-3900\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-3900/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-3900/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-3300] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-4200\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-4200/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-4200/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-4500\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-4500/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-3900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-4800\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-4800/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-4800/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-4200] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-5100\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-5100/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-5100/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-4500] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-5400\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-5400/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-5400/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-4800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-5700\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-5700/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-5700/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-5100] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-6000\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-6000/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-5400] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-6300\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-6300/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-6300/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-5700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-6600\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-6600/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-6600/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-6000] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-6900\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-6900/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-6900/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-6300] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-7200\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-7200/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-7200/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-6600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-7500\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-7500/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-6900] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-7800\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-7800/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-7800/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-7200] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-8100\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-8100/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-8100/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-7500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-8400\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-8400/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-8400/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-7800] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mClosed ./data.2.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.3.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.3.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.4.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-8700\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-8700/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-8700/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-8100] due to args.save_total_limit\n",
      "\u001b[34mClosed ./data.4.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.5.json.gz for reading...\u001b[0m\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 48\n",
      "\u001b[34mClosed ./data.5.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.6.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.6.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.7.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.7.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.8.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.8.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.9.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.9.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.10.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.10.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.11.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.11.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n",
      "\u001b[34mClosed ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.2.json.gz for reading...\u001b[0m\n",
      "Saving model checkpoint to distilbert-span-extraction-uncased/checkpoint-9000\n",
      "Configuration saved in distilbert-span-extraction-uncased/checkpoint-9000/config.json\n",
      "Model weights saved in distilbert-span-extraction-uncased/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [distilbert-span-extraction-uncased/checkpoint-8400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=0.10736266560024685, metrics={'train_runtime': 9452.9679, 'train_samples_per_second': 45.7, 'train_steps_per_second': 0.952, 'total_flos': 5.641626990001766e+16, 'train_loss': 0.10736266560024685, 'epoch': 18.0})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train('./distilbert-span-extraction-uncased/checkpoint-2100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b9490d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/config.json\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/pytorch_model.bin\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/training_args.bin\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/optimizer.pt\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/scheduler.pt\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/trainer_state.json\n",
      "distilbert-span-extraction-uncased/checkpoint-9000/rng_state.pth\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  680M    0   135  100  680M      0  4691k  0:02:28  0:02:28 --:--:--    35962k  0:02:20  0:00:12  0:02:08 5154k 0:02:15  0:01:19  0:00:56 5161k\n",
      "\n",
      "=========================\n",
      "\n",
      "Uploaded 1 file, 713 059 437 bytes\n",
      "\n",
      "wget https://bashupload.com/4UUhZ/Lii1V.gz\n",
      "\n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tar -zcvf distilbert-span-extraction-uncased-checkpoint-9000.tar.gz distilbert-span-extraction-uncased/checkpoint-9000\n",
    "!curl --upload-file ./distilbert-span-extraction-uncased-checkpoint-9000.tar.gz https://bashupload.com/ | cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefcb94",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch\tTraining Loss\tValidation Loss\tAccuracy\tPrecision\tRecall\n",
    "    1\t0.140800\t    0.131712\t    0.951745\t0.962096\t0.940169\n",
    "    2\t0.095600\t    0.137763\t    0.955120\t0.957821\t0.951820\n",
    "    3\t0.050900\t    0.155389\t    0.956762\t0.966651\t0.945832\n",
    "```\n",
    "We chose to use the model trained after 2 epochs because 3 seems to overfit the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5609fa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = None\n",
    "trainer = None\n",
    "trained = None\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def softmax_outputs(outputs) -> dict:\n",
    "    return torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "\n",
    "trained = AutoModelForQuestionAnswering.from_pretrained('./distilbert-span-extraction-uncased/checkpoint-9000')\n",
    "trained.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70607e43",
   "metadata": {},
   "source": [
    "# Run on full video transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67da4c30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mOpening ./data.16.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual (1, 39)\n",
      "Predicted (tensor(1), tensor(55))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 5.88235294117647}\n",
      "Actual (388, 510)\n",
      "Predicted (tensor(399), tensor(510))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 2.0725388601036268}\n",
      "Actual (11, 24)\n",
      "Predicted (tensor(9), tensor(27))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (30, 77)\n",
      "Predicted (tensor(1), tensor(77))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 5.714285714285714}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (380, 510)\n",
      "Predicted (tensor(424), tensor(510))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.6269592476489028}\n",
      "Actual (466, 482)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (36, 98)\n",
      "Predicted (tensor(30), tensor(95))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 11)\n",
      "Predicted (tensor(1), tensor(26))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (387, 510)\n",
      "Predicted (tensor(379), tensor(510))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 2.6666666666666665}\n",
      "Actual (92, 117)\n",
      "Predicted (tensor(96), tensor(115))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (9, 201)\n",
      "Predicted (tensor(17), tensor(258))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 5.780346820809249}\n",
      "Actual (264, 510)\n",
      "Predicted (tensor(265), tensor(352))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (9, 37)\n",
      "Predicted (tensor(1), tensor(46))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 7.4074074074074066}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 15)\n",
      "Predicted (tensor(1), tensor(24))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 48)\n",
      "Predicted (tensor(1), tensor(62))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 9.75609756097561}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 44)\n",
      "Predicted (tensor(1), tensor(42))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 5.405405405405405}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (102, 269)\n",
      "Predicted (tensor(107), tensor(272))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 3.9473684210526314}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (240, 267)\n",
      "Predicted (tensor(178), tensor(270))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 14)\n",
      "Predicted (tensor(1), tensor(52))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 25)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (163, 230)\n",
      "Predicted (tensor(169), tensor(229))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 3.0303030303030303}\n",
      "Actual (143, 164)\n",
      "Predicted (tensor(1), tensor(16))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 31)\n",
      "Predicted (tensor(1), tensor(37))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 24)\n",
      "Predicted (tensor(1), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (83, 367)\n",
      "Predicted (tensor(84), tensor(364))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 3.2}\n",
      "Actual (375, 510)\n",
      "Predicted (tensor(404), tensor(510))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 106)\n",
      "Predicted (tensor(34), tensor(106))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 15.873015873015872}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 29)\n",
      "Predicted (tensor(1), tensor(18))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (1, 16)\n",
      "Predicted (tensor(1), tensor(510))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 82.75862068965517}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(0), tensor(0))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 1.3071895424836601}\n",
      "Actual (0, 0)\n",
      "Predicted (tensor(1), tensor(87))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n",
      "Actual (192, 320)\n",
      "Predicted (tensor(182), tensor(260))\n",
      "---------\n",
      "{'exact_match': 0.0, 'f1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric('squad')\n",
    "\n",
    "def evaluate(text, start_char_idx, end_char_idx):\n",
    "#     print([text, text[start_char:end_char + 1]])\n",
    "\n",
    "    inputs = tokenize_function(text)\n",
    "\n",
    "    start_position = index_of_token(inputs['offset_mapping'], start_char_idx, default_value=0)\n",
    "    end_position = index_of_token(inputs['offset_mapping'], end_char_idx, default_value=0)\n",
    "    if start_position != 0 and end_position == 0:\n",
    "        end_position = len(inputs['input_ids']) - 2\n",
    "\n",
    "    outputs = trained(input_ids=torch.tensor([inputs['input_ids']]).cuda())\n",
    "\n",
    "    pred_start_position = torch.argmax(outputs.start_logits).cpu()\n",
    "    pred_end_position = torch.argmax(outputs.end_logits).cpu()\n",
    "    \n",
    "    pred_char_start_idx = index_of_token(inputs['offset_mapping'], pred_start_position, default_value=0)\n",
    "    pred_char_end_idx = index_of_token(inputs['offset_mapping'], pred_end_position, default_value=0)\n",
    "    \n",
    "    predicted_answers = [{\n",
    "        'id': 0,\n",
    "        'prediction_text': text[pred_char_start_idx:pred_char_end_idx+1],\n",
    "    }]\n",
    "    \n",
    "    theoretical_answers = [{\n",
    "        'id': 0,\n",
    "        'answers': [{\n",
    "            'answer_start': start_char_idx,\n",
    "            'text':text[start_char_idx:end_char_idx+1]\n",
    "        }],\n",
    "    }]\n",
    "\n",
    "    print('Actual', (start_position, end_position))\n",
    "    print('Predicted', (pred_start_position, pred_end_position))\n",
    "    print('---------')\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "    \n",
    "for inputs in itertools.islice(SpanExtractionDataset(load_data_from_chunks('data', './', [16])), 50):\n",
    "    print(evaluate(*inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e00c5387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "from data_loader import Caption, load_captions_from_chunks, segment_text, get_intersection_range\n",
    "\n",
    "def caption_times(c):\n",
    "    return c.start, c.end\n",
    "\n",
    "def prediction_times(p):\n",
    "    return tuple(p[0])\n",
    "\n",
    "def tumbling_time_window(captions, duration, key=caption_times):\n",
    "    results = [captions[0]]\n",
    "    for caption in captions:\n",
    "        if key(results[-1])[1] - key(results[0])[0] <= duration:\n",
    "            results.append(caption)\n",
    "        else:\n",
    "            yield results\n",
    "            results = [caption]\n",
    "\n",
    "    yield results\n",
    "    \n",
    "def session_time_window(captions, duration, key=caption_times):\n",
    "    captions_iter = iter(captions)\n",
    "    results = [next(captions_iter)]\n",
    "    for caption in captions_iter:\n",
    "        if key(results[-1])[1] - key(caption)[0] <= duration:\n",
    "            results.append(caption)\n",
    "        else:\n",
    "            yield results\n",
    "            results = [caption]\n",
    "\n",
    "    yield results\n",
    "\n",
    "def batch(iterable, n):\n",
    "    length = len(iterable)\n",
    "    for i in range(0, length, n):\n",
    "        yield iterable[i:min(i + n, length)]\n",
    "        \n",
    "def decode_label(outputs):\n",
    "    content, sponsor = outputs\n",
    "    \n",
    "    prediction_dict = {'sponsor': sponsor, 'content': content}\n",
    "    prediction_dict = {k: v for k, v in sorted(prediction_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    return next(iter(prediction_dict.items()))\n",
    "        \n",
    "def predict_in_batches(texts, batch_size: int = 8):    \n",
    "    batches = list(batch(texts, batch_size))\n",
    "    for b in batches:\n",
    "        inputs = defaultdict(list)\n",
    "        for text in b:\n",
    "            tokenized = tokenize_function({ 'text': text })\n",
    "            for k, v in tokenized.items():\n",
    "                inputs[k].append(v)\n",
    "            \n",
    "        inputs = { k: torch.tensor(v).cuda() for k, v in inputs.items() }\n",
    "        outputs = trained(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1).tolist()\n",
    "        yield from predictions\n",
    "        \n",
    "def predict_sponsor_segments(captions, window_duration=10):\n",
    "    windows = list(tumbling_time_window(captions, window_duration))\n",
    "    window_texts = [segment_text(window) for window in windows]\n",
    "    predictions = predict_in_batches(window_texts, 4)\n",
    "    \n",
    "    for window, text, prediction in zip(windows, window_texts, predictions):\n",
    "        yield [window[0].start, window[-1].end], text, *decode_label(prediction)\n",
    "        \n",
    "def merge_prediction_(predictions):\n",
    "    assert len(set((label for _, _, label, _ in predictions))) == 1\n",
    "    # All co-occurring predictions have the same label so we merge them\n",
    "    merged_start, merged_end = predictions[0][0][0], predictions[-1][0][1]\n",
    "    merged_text = ' '.join((text for _, text, _, _ in predictions))\n",
    "    # Don't know what the correct way to compute the joint probability here is,\n",
    "    # just assume they are independent; We don't really use this number anywhere\n",
    "    prob = np.prod([prob for _, _, _, prob in predictions])\n",
    "    return [merged_start, merged_end], merged_text, predictions[0][2], prob\n",
    "\n",
    "def merge_predictions(predictions, within_duration=5):\n",
    "    for co_occuring in session_time_window(predictions, within_duration, key=prediction_times):\n",
    "        merged = [co_occuring[0]]\n",
    "        for times, text, label, prob in co_occuring[1:]:\n",
    "            _, _, prev_label, _ = merged[0]\n",
    "            if label == prev_label:\n",
    "                merged.append((times, text, label, prob))\n",
    "            else:\n",
    "                yield merge_prediction_(merged)\n",
    "                merged = [(times, text, label, prob)]\n",
    "        \n",
    "        if len(merged) > 0:\n",
    "            yield merge_prediction_(merged)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "95705135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def range_equals(left: 'Tuple[float, float]', right: 'Tuple[float, float]', eps: float) -> bool:\n",
    "    left_start, left_end = left\n",
    "    right_start, right_end = right\n",
    "    \n",
    "    return (abs(left_start - right_start) <= eps\n",
    "        and abs(left_end - right_end) <= eps)\n",
    "\n",
    "def count_range_equals(pairs, eps: float) -> int:\n",
    "    cnt = 0\n",
    "    for left, right in pairs:\n",
    "        if range_equals(left, right, eps):\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "assert range_equals([0, 5], [0, 5], eps=0)\n",
    "assert range_equals([1, 6], [0, 5], eps=1)\n",
    "assert range_equals([-1, 4], [0, 5], eps=1)\n",
    "assert not range_equals([-2, 4], [0, 5], eps=1)\n",
    "assert not range_equals([1, 7], [0, 5], eps=1)\n",
    "\n",
    "def range_negation(base: 'Tuple[float, float]', ranges: 'List[Tuple[float, float]]') -> 'List[Tuple[float, float]]':\n",
    "    \"\"\"\n",
    "    base:    |-------------|\n",
    "    ranges:  | ***   **    |\n",
    "    Return:  |#   ###  ####|\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    last_end = base[0]\n",
    "    for r in ranges:\n",
    "        if last_end != r[0]:\n",
    "            results.append((last_end, r[0]))\n",
    "        last_end = r[1]\n",
    "    if last_end != base[1]:\n",
    "        results.append((last_end, base[1]))\n",
    "        \n",
    "    return results\n",
    "    \n",
    "assert range_negation((2, 10), [(3,4), (5, 6)]) == [(2, 3), (4, 5), (6, 10)]\n",
    "assert range_negation((2, 6), [(3,4), (5, 6)]) == [(2, 3), (4, 5)]\n",
    "assert range_negation((3, 6), [(3,4), (5, 6)]) == [(4, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49cff8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34mFound ./data.1.json.gz.\u001b[0m\n",
      "\u001b[34mOpening ./data.1.json.gz for reading...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping --6T95cQa50 because sponsor times do not match the captions\n",
      "Dropping --BXjAWlPDQ because sponsor times do not match the captions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd20e20518104c44b3e823ecfc468124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[45m---jcia5ufM [[28, 45]]\u001b[0m\n",
      "\tPredicted=[(40, 45), (204, 208)],\n",
      "\tExpected=[[28, 45]]\n",
      "\u001b[45m--4bRr1Pwlg [[28, 56]]\u001b[0m\n",
      "\tPredicted=[(33, 42), (49, 52), (323, 326)],\n",
      "\tExpected=[[28, 56]]\n",
      "\u001b[45m--4EqGOaEgU [[40, 54]]\u001b[0m\n",
      "\tPredicted=[],\n",
      "\tExpected=[[40, 54]]\n",
      "\u001b[45m--540zBQ6GI [[0, 5]]\u001b[0m\n",
      "\tPredicted=[(0, 4)],\n",
      "\tExpected=[[0, 5]]\n",
      "\u001b[45m--6CCgW32LE [[0, 29]]\u001b[0m\n",
      "\tPredicted=[(0, 5), (19, 32), (81, 86)],\n",
      "\tExpected=[[0, 29]]\n",
      "\u001b[45m--B_ZkOUCDc [[0, 2]]\u001b[0m\n",
      "\tPredicted=[(0, 2), (116, 126)],\n",
      "\tExpected=[[0, 2]]\n",
      "\u001b[45m--CWTjd8rkY [[746, 826]]\u001b[0m\n",
      "\tPredicted=[(543, 548), (747, 764), (771, 776), (784, 819)],\n",
      "\tExpected=[[746, 826]]\n",
      "\u001b[45m--cYDnBVfvE [[0, 0]]\u001b[0m\n",
      "\tPredicted=[],\n",
      "\tExpected=[[0, 0]]\n",
      "\u001b[45m--JOtw1cCso [[25, 46]]\u001b[0m\n",
      "\tPredicted=[(24, 47), (445, 452)],\n",
      "\tExpected=[[25, 46]]\n",
      "\u001b[45m--JVap-3nJU [[0, 4]]\u001b[0m\n",
      "\tPredicted=[(0, 2), (127, 129), (153, 153), (614, 624)],\n",
      "\tExpected=[[0, 4]]\n",
      "Exact match (with 5s threshold) 0.08333333333333333\n",
      "Confusion matrix [[27189   360]\n",
      " [  510   821]]\n",
      "Accuracy 0.9698753462603878\n",
      "Precision 0.6951735817104149\n",
      "Recall 0.6168294515401953\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def create_labels_from_range(captions, sponsor_ranges):\n",
    "    caption_labels = np.zeros(len(captions), dtype=bool)\n",
    "    for start_idx, end_idx in sponsor_ranges:\n",
    "        for i in range(start_idx, end_idx + 1):\n",
    "            caption_labels[i] = True\n",
    "\n",
    "    token_labels = []\n",
    "    for i, caption in enumerate(captions):\n",
    "        num_tokens = len(caption.text.split())\n",
    "        token_labels.extend([caption_labels[i]] * num_tokens)\n",
    "    return token_labels\n",
    "\n",
    "def create_labels_from_times(captions, sponsor_times):\n",
    "    ranges = [get_intersection_range(captions, *pair[1]) for pair in sponsor_times]\n",
    "    return create_labels_from_range(captions, ranges)\n",
    "\n",
    "def evaluate(videos, eps=5):\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    predicted_labels = np.empty(0)\n",
    "    actual_labels = np.empty(0)\n",
    "    # Values for our close match metric (exact match with threshold)\n",
    "    # Number of maches\n",
    "    close_matches = 0\n",
    "    # Number of predicted ranges\n",
    "    total_predicted_ranges = 0\n",
    "    \n",
    "    for video_id, captions, sponsor_ranges in tqdm(videos):\n",
    "        print(colored(f'{video_id} {sponsor_ranges}', None, 'on_magenta'))\n",
    "        sponsor_times = [(captions[start].start, captions[end].end) for start, end in sponsor_ranges]\n",
    "        predicted_sponsor_times = []\n",
    "\n",
    "        for times, text, label, prob in merge_predictions(predict_sponsor_segments(captions, window_duration=10), within_duration=10):\n",
    "            if label == 'sponsor':\n",
    "                predicted_sponsor_times.append((f'{int(prob * 100)}%', times))\n",
    "\n",
    "            color = { 'sponsor': 'yellow', 'content': None }[label]\n",
    "            # print(colored(f'{int(prob * 100)}% {times[0]} <--> {times[1]} {text}', color=color))\n",
    "            \n",
    "            if any((range_equals(times, actual_times, eps) for actual_times in sponsor_times)):\n",
    "                close_matches += 1\n",
    "            total_predicted_ranges += 1\n",
    "\n",
    "        predicted_sponsor_ranges = [get_intersection_range(captions, *pair[1]) for pair in predicted_sponsor_times]\n",
    "        predicted_labels = np.append(predicted_labels, create_labels_from_range(captions, predicted_sponsor_ranges))\n",
    "        actual_labels = np.append(actual_labels, create_labels_from_range(captions, sponsor_ranges))\n",
    "        \n",
    "        print(f'\\tPredicted={predicted_sponsor_ranges},\\n\\tExpected={sponsor_ranges}')\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "    \n",
    "    close_match_score = close_matches / total_predicted_ranges\n",
    "    print(f'Exact match (with {eps}s threshold)', close_match_score)\n",
    "    print('Confusion matrix', confusion_matrix(actual_labels, predicted_labels))\n",
    "    print('Accuracy', accuracy_score(actual_labels, predicted_labels))\n",
    "    print('Precision', precision_score(actual_labels, predicted_labels))\n",
    "    print('Recall', recall_score(actual_labels, predicted_labels))\n",
    "        \n",
    "evaluate(list(itertools.islice(load_captions_from_chunks('data', './', [1]), 0, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699f0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb57b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
