{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fjRr3oE_3Q26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: pandas in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.1.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.6.0)\n",
      "Requirement already satisfied: packaging in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (20.4)\n",
      "Requirement already satisfied: aiohttp in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.24.0)\n",
      "Requirement already satisfied: dill in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2022.3.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: urllib3>=1.25.10 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from responses<0.19->datasets) (1.25.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyyaml in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.3.1)\n",
      "Requirement already satisfied: filelock in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: six in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: transformers in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2020.10.15)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: sacremoses in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: filelock in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: six in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: seqeval in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from seqeval) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from seqeval) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/hisham/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wjBtwAyE3UnQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "from data_loader import GzippedJSONDataset, LabelledCaptionsDataset, get_intersection_range, load_captions_from_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized, labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5jZqHZgm3XtK"
   },
   "outputs": [],
   "source": [
    "class LabelledTokensDataset(torch.utils.data.IterableDataset):\n",
    "    \"\"\"\n",
    "    IterableDataset that tokenizes the transcripts of a given caption dataset\n",
    "    and labels the tokens according to whether they are included in a sponsor-labeled caption.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: torch.utils.data.IterableDataset, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __iter__(self):\n",
    "        for video_id, captions, sponsor_times in self.dataset:\n",
    "\n",
    "            drop_row = False\n",
    "\n",
    "            sponsor_ranges = []\n",
    "\n",
    "            for start_time, end_time in sponsor_times:\n",
    "                # get intersection range and extract the sponsor text from it\n",
    "                start_index, end_index = get_intersection_range(captions, start_time, end_time)\n",
    "                if start_index is None or end_index is None:\n",
    "                    print(f'Dropping {video_id} because sponsor times do not match the captions')\n",
    "                    drop_row = True\n",
    "                    break\n",
    "\n",
    "                # mark range as sponsor\n",
    "                for i in range(start_index, end_index):\n",
    "                    captions[i]['is_sponsor'] = True\n",
    "\n",
    "                sponsor_ranges.append([start_index, end_index])\n",
    "\n",
    "            if not drop_row:\n",
    "                input_ids = []\n",
    "                labels = []\n",
    "\n",
    "                for caption in captions:\n",
    "                    tokenized_caption = self.tokenizer(caption['text'])\n",
    "                    # remove special beginning/end tokens\n",
    "                    input_ids += tokenized_caption['input_ids'][1:-1]\n",
    "                    \n",
    "                    # label every token accordingly\n",
    "                    label = 1 if 'is_sponsor' in caption else 0\n",
    "                    labels += [label] * len(tokenized_caption['input_ids'][1:-1])\n",
    "                \n",
    "                # flag indicating whether a completely non-sponsor segment has been yielded.\n",
    "                # limitting the number of fully non-sponsor segments to balance the data\n",
    "                yielded_non_sponsor = False\n",
    "                \n",
    "                # go through the transcript max_length segment by max_length segment\n",
    "                for window_start in range(0, len(input_ids), 510):\n",
    "                    w_input_ids = input_ids[window_start:]\n",
    "                    w_labels = labels[window_start:]\n",
    "                    \n",
    "                    # make sure to yield at most 1 completely non-sponsor segment\n",
    "                    if 1 not in w_labels:\n",
    "                        if yielded_non_sponsor:\n",
    "                            continue\n",
    "                        else:\n",
    "                            yielded_non_sponsor = True\n",
    "                            \n",
    "                    # add back special tokens\n",
    "                    prepared_tokenizer = self.tokenizer.prepare_for_model(w_input_ids, truncation=True, padding='max_length')\n",
    "\n",
    "                    attention_mask = prepared_tokenizer['attention_mask']\n",
    "\n",
    "                    # loop to deal with special tokens, labelling them with -100 for the BERT model to ignore\n",
    "                    new_labels = []\n",
    "                    for i, m in enumerate(attention_mask):\n",
    "                        if m == 1:\n",
    "                            if i == 0:\n",
    "                                new_labels.append(-100)\n",
    "                            elif i == len(attention_mask) - 1:\n",
    "                                new_labels.append(-100)\n",
    "                            elif i == len(w_input_ids) + 1:\n",
    "                                new_labels.append(-100)\n",
    "                            else:\n",
    "                                new_labels.append(w_labels[i-1])\n",
    "                        else:\n",
    "                            new_labels.append(-100)\n",
    "\n",
    "                    w_input_ids = prepared_tokenizer['input_ids']\n",
    "\n",
    "                    yield {'input_ids': w_input_ids, 'labels': new_labels, 'attention_mask': attention_mask}\n",
    "\n",
    "    def __len__(self):\n",
    "        # needed for training, 20001 is the length of every chunk in the caption dataset\n",
    "        return 20001\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset'\n",
    "train_dataset = load_captions_from_chunks('data', root_dir=dataset_dir, chunks=range(1, 3))\n",
    "eval_dataset = GzippedJSONDataset(f'{dataset_dir}/data.16.json.gz', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising training and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmvQWT7qKDDj",
    "outputId": "979e849f-105c-4dad-9174-7c9d382db59f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "batch_size = 4\n",
    "\n",
    "# Initialise tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "# tokenize and label datasets\n",
    "labelled_train_dataset = LabelledTokensDataset(train_dataset, tokenizer)\n",
    "\n",
    "labelled_eval_dataset = LabelledTokensDataset(eval_dataset, tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test_sponsors\",\n",
    "    save_steps=500,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.00001,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "\n",
    "# use SeqEval as the evaluation library\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "# define which metrics will be reported\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# specify components of the training and evaluation processes\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=labelled_train_dataset,\n",
    "    eval_dataset=labelled_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "T_z_O1arIIMq",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ff2b3ca4-7ae2-4ae9-f3a2-253c4fb87820"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hisham/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20001\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3753\n"
     ]
    }
   ],
   "source": [
    "# train, evaluate and save the model\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model('seq_labelling.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sponsor_range(model, tokenizer, captions):\n",
    "    token_captions = []\n",
    "    input_ids = []\n",
    "    for i, caption in enumerate(captions):\n",
    "        tokenized_caption = tokenizer(caption['text'], add_special_tokens=False)['input_ids']\n",
    "        input_ids += tokenized_caption\n",
    "        token_captions += [i] * len(tokenized_caption)\n",
    "\n",
    "    predicted_labels = []\n",
    "    for window_start in range(0, len(input_ids), 512):\n",
    "        if window_start + 512 < len(input_ids):\n",
    "            w_input_ids = input_ids[window_start:window_start + 512]\n",
    "        else:\n",
    "            w_input_ids = input_ids[window_start:]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model.forward(input_ids=torch.tensor(w_input_ids).unsqueeze(0))\n",
    "            # softmax is applied on the outputs of the previous step\n",
    "            predictions = list(torch.argmax(predictions.logits.squeeze(), axis=1))\n",
    "\n",
    "        predicted_labels += predictions\n",
    "\n",
    "        \n",
    "    predicted_ranges = []\n",
    "    in_sponsor = False\n",
    "    start = -1\n",
    "    for i, label in enumerate(predicted_labels):\n",
    "        if label == 1:\n",
    "            if not in_sponsor:\n",
    "                in_sponsor = True\n",
    "                start = i\n",
    "        elif label == 0:\n",
    "            if in_sponsor:\n",
    "                predicted_ranges.append((token_captions[start], token_captions[i-1]))\n",
    "            in_sponsor = False\n",
    "    else:\n",
    "        if in_sponsor:\n",
    "            predicted_ranges.append((token_captions[start], token_captions[-1]))\n",
    "    \n",
    "    return predicted_ranges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file seq_labelling.model/added_tokens.json. We won't load it.\n",
      "loading file seq_labelling.model/vocab.txt\n",
      "loading file seq_labelling.model/tokenizer.json\n",
      "loading file None\n",
      "loading file seq_labelling.model/special_tokens_map.json\n",
      "loading file seq_labelling.model/tokenizer_config.json\n",
      "loading configuration file seq_labelling.model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"seq_labelling.model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file seq_labelling.model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
      "\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at seq_labelling.model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32hCmkB7VGk\n",
      "True ranges: [[0, 26]]\n",
      "Labelled ranges: []\n"
     ]
    }
   ],
   "source": [
    "# load the tokeniser\n",
    "tokenizer = AutoTokenizer.from_pretrained('seq_labelling.model')\n",
    "\n",
    "# load the fine-tuned model\n",
    "model = AutoModelForTokenClassification.from_pretrained('seq_labelling.model', num_labels=2)\n",
    "\n",
    "# get test video\n",
    "video_id, captions, sponsor_times = next(LabelledCaptionsDataset(GzippedJSONDataset(f'{dataset_dir}/data.2.json.gz')).__iter__())\n",
    "\n",
    "predicted_ranges = predict_sponsor_range(model, tokenizer, captions)\n",
    "print(video_id)\n",
    "print(f'True ranges: {sponsor_times}')\n",
    "print(f'Labelled ranges: {predicted_ranges}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SponsorML_seq_labelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
